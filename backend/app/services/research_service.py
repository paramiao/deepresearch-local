import os
import re
import time
import threading
import uuid
import random
import logging
from urllib.parse import urlparse
from flask import current_app
from app.services.siliconflow_service import SiliconFlowService
from app.services.search_service import search_service

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ResearchProcess:
    """ç ”ç©¶è¿‡ç¨‹ç±»ï¼Œç”¨äºç®¡ç†å’Œè·Ÿè¸ªç ”ç©¶è¿‡ç¨‹"""
    
    def __init__(self, topic, requirements):
        self.topic = topic
        self.requirements = requirements
        self.status = "planning"  # planning, researching, analyzing, completed, error
        self.progress = 0
        self.plan = None
        self.current_step = None
        self.research_sites = []
        self.research_findings = []
        self.analysis_results = []
        self.source_contents = {}  # å­˜å‚¨æŠ“å–çš„ç½‘é¡µå†…å®¹
        self.search_queries = []  # å­˜å‚¨æœç´¢æŸ¥è¯¢
        self.research_steps = []  # å­˜å‚¨è¯¦ç»†çš„ç ”ç©¶æ­¥éª¤åŠå…¶è¿›åº¦å’Œç»“æœ
        self.current_step_index = 0  # å½“å‰æ‰§è¡Œåˆ°çš„æ­¥éª¤ç´¢å¼•
        self.report = None
        self.error = None
        self.process_id = str(int(time.time() * 1000))
        self.start_time = time.time()
        self.ai_service = SiliconFlowService()
        logger.info(f"åˆå§‹åŒ–ç¡…åŸºæµåŠ¨APIæœåŠ¡ç”¨äºç ”ç©¶è¿‡ç¨‹: {self.process_id}")
        
    def to_dict(self):
        """å°†ç ”ç©¶è¿‡ç¨‹è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "process_id": self.process_id,
            "topic": self.topic,
            "requirements": self.requirements,
            "status": self.status,
            "progress": self.progress,
            "plan": self.plan,
            "current_step": self.current_step,
            "current_step_index": self.current_step_index,
            "research_steps": self.research_steps,  # æ·»åŠ è¯¦ç»†çš„ç ”ç©¶æ­¥éª¤åˆ—è¡¨
            "research_sites": self.research_sites,
            "research_findings": self.research_findings[:15],  # å¢åŠ è¿”å›çš„å‘ç°æ•°é‡
            "analysis_results": self.analysis_results,
            "search_queries": self.search_queries[:5],  # è¿”å›çš„æŸ¥è¯¢æ•°é‡
            "report": self.report,
            "error": self.error,
            "elapsed_time": round(time.time() - self.start_time, 2)
        }

class ResearchService:
    """ç ”ç©¶æœåŠ¡ï¼Œç”¨äºç®¡ç†æ‰€æœ‰ç ”ç©¶è¿‡ç¨‹"""
    
    def __init__(self):
        self.research_processes = {}
        # ä¸å†é¢„å®šä¹‰ç½‘ç«™åˆ—è¡¨ï¼Œè€Œæ˜¯ä½¿ç”¨æœç´¢æœåŠ¡æ¥è·å–çœŸå®æ•°æ®
        logger.info("åˆå§‹åŒ–ç ”ç©¶æœåŠ¡ï¼Œä½¿ç”¨çœŸå®æ•°æ®æ¨¡å¼")
        
    def create_research_process(self, topic, requirements):
        """åˆ›å»ºæ–°çš„ç ”ç©¶è¿‡ç¨‹"""
        research_process = ResearchProcess(topic, requirements)
        self.research_processes[research_process.process_id] = research_process
        self._start_research_thread(research_process)
        return research_process.process_id
        
    def get_research_process(self, process_id):
        """è·å–ç ”ç©¶è¿‡ç¨‹"""
        return self.research_processes.get(process_id)
        
    def _start_research_thread(self, research_process):
        """å¯åŠ¨ç ”ç©¶è®¡åˆ’ç”Ÿæˆçº¿ç¨‹ï¼Œåªè´Ÿè´£ç”Ÿæˆè®¡åˆ’ï¼Œä¸æ‰§è¡Œå®Œæ•´ç ”ç©¶"""
        thread = threading.Thread(target=self._generate_research_plan, args=(research_process,))
        thread.daemon = True
        thread.start()
        
    def _generate_research_plan(self, process):
        """åªç”Ÿæˆç ”ç©¶è®¡åˆ’ï¼Œä¸æ‰§è¡Œå®Œæ•´ç ”ç©¶è¿‡ç¨‹"""
        try:
            # 1. ç”Ÿæˆç ”ç©¶è®¡åˆ’
            process.status = "planning"
            process.current_step = "ç”Ÿæˆç ”ç©¶è®¡åˆ’"
            
            try:
                process.plan = process.ai_service.generate_research_plan(
                    process.topic, process.requirements
                )
                logger.info("ç ”ç©¶è®¡åˆ’ç”ŸæˆæˆåŠŸ")
                process.progress = 20
            except Exception as e:
                logger.error(f"ç”Ÿæˆç ”ç©¶è®¡åˆ’æ—¶å‡ºé”™: {str(e)}")
                process.error = f"ç”Ÿæˆç ”ç©¶è®¡åˆ’æ—¶å‡ºé”™: {str(e)}"
                process.status = "error"
                return
                
            # 2. ç­‰å¾…ç”¨æˆ·ç¡®è®¤ç ”ç©¶è®¡åˆ’
            process.status = "waiting_confirmation"
            process.current_step = "ç­‰å¾…ç”¨æˆ·ç¡®è®¤ç ”ç©¶è®¡åˆ’"
            
            # åç»­çš„ç ”ç©¶æ‰§è¡Œæ­¥éª¤å°†åœ¨ç”¨æˆ·ç¡®è®¤åé€šè¿‡start_research_executionè§¦å‘
            
        except Exception as e:
            process.error = f"ç”Ÿæˆç ”ç©¶è®¡åˆ’è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
            process.status = "error"
        
    def _conduct_research(self, process):
        """æŒ‰ç…§ç ”ç©¶è®¡åˆ’çš„æ¯ä¸ªæ­¥éª¤æ‰§è¡Œç ”ç©¶ï¼ˆä»…åœ¨ç”¨æˆ·ç¡®è®¤è®¡åˆ’åè¿›è¡Œï¼‰
        ä¼˜åŒ–ç‰ˆæµç¨‹ï¼šä¸“æ³¨äºæ ¸å¿ƒç ”ç©¶é—®é¢˜çš„ä¸“é—¨æ£€ç´¢å’Œåˆ†æ
        """
        try:
            # ç¡®ä¿å½“å‰çŠ¶æ€æ­£ç¡® - å…è®¸confirmedæˆ–waiting_confirmationçŠ¶æ€
            if process.status != "confirmed" and process.status != "waiting_confirmation":
                logger.error(f"çŠ¶æ€é”™è¯¯ï¼Œæ— æ³•æ‰§è¡Œç ”ç©¶: {process.status}")
                return
                
            logger.info(f"ç”¨æˆ·å·²ç¡®è®¤ç ”ç©¶è®¡åˆ’ï¼Œå¼€å§‹æ‰§è¡Œç ”ç©¶: {process.process_id}")
            
            # 1. è§£æç ”ç©¶è®¡åˆ’ä¸­çš„æ­¥éª¤ï¼Œä¼˜åŒ–ç‰ˆæœ¬ä¸“æ³¨äºç ”ç©¶é—®é¢˜
            research_steps = self._parse_research_plan(process.plan, prioritize_questions=True)
            total_steps = len(research_steps)
            
            # åˆå§‹åŒ–ç ”ç©¶æ­¥éª¤æ•°æ®ç»“æ„
            process.research_steps = [
                {
                    "title": step["title"],
                    "description": step["description"],
                    "is_core_question": step.get("is_core_question", False),  # æ–°å¢å±æ€§ï¼Œæ ‡è®°æ˜¯å¦ä¸ºæ ¸å¿ƒç ”ç©¶é—®é¢˜
                    "completed": False,
                    "search_results": [],
                    "findings": [],
                    "analysis": None
                }
                for step in research_steps
            ]
            
            # æ›´æ–°çŠ¶æ€ä¸ºç ”ç©¶ä¸­
            process.status = "researching"
            process.progress = 30
            
            # 2. åŒºåˆ†æ ¸å¿ƒç ”ç©¶é—®é¢˜å’Œå…¶ä»–æ­¥éª¤
            core_research_steps = []
            knowledge_steps = []  # ç ”ç©¶ç›®æ ‡å’Œç ”ç©¶æ–¹æ³•ç­‰çŸ¥è¯†æ€§æ­¥éª¤
            
            for i, step in enumerate(research_steps):
                step_title = step["title"]
                step_description = step["description"]
                is_core_question = False
                
                # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ£€æµ‹æ–¹æ³•åˆ¤æ–­æ­¥éª¤ç±»å‹
                if step.get("is_core_question", False) or \
                   ("æ ¸å¿ƒ" in step_title and "é—®é¢˜" in step_title) or \
                   ("ç ”ç©¶é—®é¢˜" in step_title) or \
                   ("å…³é”®é—®é¢˜" in step_title) or \
                   any(q in step_title.lower() for q in ["é—®é¢˜", "question", "inquiry", "æ¢ç©¶"]):
                    is_core_question = True
                    step["is_core_question"] = True  # æ›´æ–°æ­¥éª¤å±æ€§
                    process.research_steps[i]["is_core_question"] = True  # ä¿æŒåŒæ­¥
                    core_research_steps.append(i)
                elif ("ç ”ç©¶ç›®æ ‡" in step_title) or ("ç ”ç©¶æ–¹æ³•" in step_title) or \
                     ("ç ”ç©¶èƒŒæ™¯" in step_title) or ("ç ”ç©¶èŒƒå›´" in step_title):
                    knowledge_steps.append(i)
                    # å°†è¿™äº›æ­¥éª¤æ ‡è®°ä¸ºçŸ¥è¯†æ€§æ­¥éª¤
                    step["is_knowledge_step"] = True
                    process.research_steps[i]["is_knowledge_step"] = True
                else:
                    # å…¶ä»–å®è´¨æ€§åˆ†ææ­¥éª¤ä¹Ÿè§†ä¸ºæ ¸å¿ƒ
                    core_research_steps.append(i)
                
                logger.info(f"æ­¥éª¤ {i+1}: {step_title} - {'æ ¸å¿ƒç ”ç©¶é—®é¢˜' if is_core_question else 'çŸ¥è¯†æ€§æ­¥éª¤' if i in knowledge_steps else 'åˆ†ææ­¥éª¤'} (ä¼˜å…ˆçº§: {len(core_research_steps)})")
                
                # åˆå§‹åŒ–æ­¥éª¤æ•°æ®ä¸­çš„åŸå§‹å†…å®¹
                process.research_steps[i]["original_content"] = step.get("original_content", "")  # ä¿å­˜åŸå§‹å†…å®¹
            
            # ä¼˜åŒ–ç ”ç©¶æµç¨‹ - æŒ‰ä¸åŒç±»å‹å¤„ç†æ­¥éª¤å¹¶ä¿ç•™å®Œæ•´è¿‡ç¨‹
            
            # å…ˆå¤„ç†çŸ¥è¯†æ€§æ­¥éª¤(ç ”ç©¶ç›®æ ‡ç­‰)
            for i in knowledge_steps:
                step = research_steps[i]
                step_title = step["title"]
                step_description = step["description"]
                
                process.current_step = f"ç”ŸæˆçŸ¥è¯†æ€§å†…å®¹: {step_title} ({i+1}/{total_steps})"
                logger.info(f"å¼€å§‹å¤„ç†çŸ¥è¯†æ€§æ­¥éª¤ {i+1}/{total_steps}: {step_title}")
                
                # æ›´æ–°å½“å‰æ­¥éª¤ç´¢å¼•
                process.current_step_index = i
                current_step_data = process.research_steps[i]
                
                # ä½¿ç”¨AIç›´æ¥ç”Ÿæˆæ¦‚è¿°ï¼Œè€Œéæ‰§è¡Œæœç´¢
                from app.services.siliconflow_service import siliconflow_service
                
                analysis = siliconflow_service.create_knowledge_content(
                    step_title,
                    step_description,
                    process.topic,
                    process.plan
                )
                
                current_step_data["analysis"] = analysis
                logger.info(f"ç”Ÿæˆäº†çŸ¥è¯†æ€§æ­¥éª¤ '{step_title}' çš„å†…å®¹")
                current_step_data["completed"] = True
                
                # æ›´æ–°è¿›åº¦
                progress_increment = 10 / len(knowledge_steps) if len(knowledge_steps) > 0 else 0
                process.progress = 30 + progress_increment
            
            # å¤„ç†æ ¸å¿ƒç ”ç©¶é—®é¢˜ - æ¯ä¸ªé—®é¢˜å•ç‹¬æœç´¢å¹¶ç”Ÿæˆåˆ†æ
            logger.info(f"å¼€å§‹å¤„ç†æ ¸å¿ƒç ”ç©¶é—®é¢˜ï¼Œå…± {len(core_research_steps)} ä¸ªé—®é¢˜")
            
            for idx, i in enumerate(core_research_steps):
                step = research_steps[i]
                step_title = step["title"]
                step_description = step["description"]
                
                process.current_step = f"ç ”ç©¶é—®é¢˜: {step_title} ({idx+1}/{len(core_research_steps)})"
                logger.info(f"å¼€å§‹æ‰§è¡Œæ ¸å¿ƒç ”ç©¶é—®é¢˜ {idx+1}/{len(core_research_steps)}: {step_title}")
                
                # æ›´æ–°å½“å‰æ­¥éª¤ç´¢å¼•
                process.current_step_index = i
                current_step_data = process.research_steps[i]
                current_step_data["step_number"] = i+1  # ä¿å­˜æ­¥éª¤ç¼–å·
                current_step_data["search_queries"] = []  # åˆå§‹åŒ–æ­¥éª¤çš„æœç´¢æŸ¥è¯¢åˆ—è¡¨
                step_findings = []
                
                # ç”Ÿæˆå½“å‰ç ”ç©¶é—®é¢˜çš„æœç´¢æŸ¥è¯¢
                search_queries = self._generate_step_search_queries(step_title, step_description, process.topic)
                logger.info(f"ä¸ºæ ¸å¿ƒç ”ç©¶é—®é¢˜ '{step_title}' ç”Ÿæˆäº† {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
                
                # å­˜å‚¨æ­¥éª¤çš„æœç´¢æŸ¥è¯¢ï¼Œä¾¿äºå‰ç«¯å±•ç¤º
                current_step_data["search_queries"] = search_queries
                
                # æ·»åŠ åˆ°æ€»æŸ¥è¯¢åˆ—è¡¨ä¸­
                process.search_queries.extend(search_queries)
                
                # å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰§è¡Œæœç´¢
                for query_idx, query in enumerate(search_queries):
                    process.current_step = f"æœç´¢: '{query}' (é—®é¢˜ {idx+1}/{len(core_research_steps)}, æŸ¥è¯¢ {query_idx+1}/{len(search_queries)})"
                    logger.info(f"æ‰§è¡Œæœç´¢æŸ¥è¯¢ {query_idx+1}/{len(search_queries)}: {query}")
                    
                    # è°ƒç”¨æœç´¢æœåŠ¡
                    search_results = search_service.search(query)
                    logger.info(f"æŸ¥è¯¢ '{query}' è¿”å›äº† {len(search_results)} ä¸ªç»“æœ")
                    
                    # ä¸ºæŸ¥è¯¢åˆ›å»ºç»“æœç»“æ„
                    query_result = {
                        "query": query,
                        "results": [],
                        "findings": []
                    }
                    
                    # å¤„ç†æœç´¢ç»“æœ
                    for result in search_results:
                        site_info = {
                            "name": result["source"],
                            "url": result["link"],
                            "title": result["title"],
                            "snippet": result["snippet"],
                            "icon": result.get("source_icon", "ğŸ”")
                        }
                        
                        # æ·»åŠ åˆ°å½“å‰æŸ¥è¯¢çš„ç»“æœä¸­
                        query_result["results"].append(site_info)
                        
                        # æ·»åŠ åˆ°æ­¥éª¤çš„æ€»æœç´¢ç»“æœä¸­
                        if site_info not in current_step_data["search_results"]:
                            current_step_data["search_results"].append(site_info)
                        
                        # åŒæ—¶æ·»åŠ åˆ°æ€»çš„ç ”ç©¶ç½‘ç«™åˆ—è¡¨ä¸­
                        if site_info not in process.research_sites:
                            process.research_sites.append(site_info)
                    
                    # ä»æœç´¢ç»“æœæå–å†…å®¹
                    extracted_findings = []
                    for result_idx, result in enumerate(search_results[:3]):  # æ¯ä¸ªæŸ¥è¯¢é€‰æ‹©3ä¸ªç»“æœæ·±å…¥åˆ†æ
                        try:
                            url = result["link"]
                            process.current_step = f"æå–å†…å®¹: {result['source']} (é—®é¢˜ {idx+1}, æŸ¥è¯¢ {query_idx+1})"
                            
                            # è·å–ç½‘é¡µå†…å®¹
                            content = search_service.fetch_content(url)
                            if not content:
                                continue
                                
                            # å­˜å‚¨ç½‘é¡µå†…å®¹
                            process.source_contents[url] = content
                            
                            # æå–ç›¸å…³ä¿¡æ¯
                            key_info = search_service.extract_key_information(content, query)
                        
                            # åªæœ‰å½“key_infoæœ‰æ•ˆä¸”ä¸ä¸ºNoneæ—¶æ‰åˆ›å»ºå‘ç°
                            if key_info:
                                # æ ¼å¼åŒ–å‘ç°å†…å®¹ï¼Œå¢å¼ºå¯è¯»æ€§
                                domain = urlparse(result['link']).netloc
                                finding = f"æ ¹æ®{result['source']}({domain})çš„æ•°æ®ï¼Œ{key_info}"
                                
                                # æ·»åŠ åˆ°å½“å‰æŸ¥è¯¢çš„å‘ç°ä¸­
                                query_result["findings"].append(finding)
                                extracted_findings.append(finding)
                                
                                # æ·»åŠ åˆ°æ­¥éª¤çš„å‘ç°ä¸­
                                if finding not in current_step_data["findings"]:
                                    current_step_data["findings"].append(finding)
                                    logger.info(f"æ·»åŠ æ–°çš„ç ”ç©¶å‘ç°: {finding[:100]}...")
                                    
                                # åŒæ—¶æ·»åŠ åˆ°æ€»çš„ç ”ç©¶å‘ç°ä¸­
                                if finding not in process.research_findings:
                                    process.research_findings.append(finding)
                                    step_findings.append(finding)
                            else:
                                logger.warning(f"æ— æ³•ä» {result['source']} æå–æœ‰æ•ˆä¿¡æ¯")
                        except Exception as e:
                            logger.error(f"è·å–ç½‘é¡µå†…å®¹æ—¶å‡ºé”™: {str(e)}")
                    
                    # å°†å½“å‰æŸ¥è¯¢çš„ç»“æœå­˜å‚¨åˆ°æ­¥éª¤ä¸­
                    if not hasattr(current_step_data, "query_results"):
                        current_step_data["query_results"] = []
                    current_step_data["query_results"].append(query_result)
                    
                    # ä¸ºå½“å‰æŸ¥è¯¢ç”Ÿæˆå°ç»“
                    if extracted_findings:
                        query_summary = self._generate_query_summary(query, extracted_findings, step_title)
                        query_result["summary"] = query_summary
                        logger.info(f"ä¸ºæŸ¥è¯¢ '{query}' ç”Ÿæˆäº†å°ç»“")

                
                # ä½¿ç”¨LLMåˆ†æè¯¥æ­¥éª¤çš„ç»“æœ
                if step_findings:
                    process.current_step = f"åˆ†ææ­¥éª¤ {i+1} çš„å‘ç°: {step_title}"
                    step_analysis = process.ai_service.analyze_step_findings(
                        step_title, 
                        "\n".join(step_findings)
                    )
                    
                    # ä¿å­˜åˆ†æç»“æœ
                    current_step_data["analysis"] = step_analysis
                    process.analysis_results.append(f"**{step_title}**\n{step_analysis}")
                else:
                    # å¦‚æœæ²¡æœ‰å‘ç°ï¼Œä¹Ÿéœ€è¦æ·»åŠ ä¸€ä¸ªç©ºçš„åˆ†æç»“æœ
                    current_step_data["analysis"] = "æœªæ”¶é›†åˆ°è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œåˆ†æã€‚"
                    process.analysis_results.append(f"**{step_title}**\næœªæ”¶é›†åˆ°è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œåˆ†æã€‚")
                
                # æ ‡è®°æ­¥éª¤å®Œæˆ
                current_step_data["completed"] = True
                
                # æ›´æ–°è¿›åº¦
                process.progress = 30 + (i+1) * (50 / total_steps)
            
            # 3. ç”ŸæˆæŠ¥å‘Šé˜¶æ®µ
            process.status = "reporting"
            process.current_step = "ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"
            process.progress = 90
            
            try:
                # æ”¶é›†æ‰€æœ‰æ­¥éª¤çš„ç ”ç©¶å‘ç°å’Œåˆ†æç»“æœ - å…¨é¢å¢å¼ºç‰ˆ
                findings_text = ""
                
                # æ”¶é›†æ‰€æœ‰æ ¸å¿ƒç ”ç©¶é—®é¢˜çš„è¯¦ç»†å‘ç°å’Œåˆ†æ
                core_questions = []
                for step_data in process.research_steps:
                    if step_data.get("is_core_question", False):
                        core_questions.append(step_data)
                
                logger.info(f"ä¸ºç ”ç©¶æŠ¥å‘Šæ”¶é›†äº† {len(core_questions)} ä¸ªæ ¸å¿ƒç ”ç©¶é—®é¢˜çš„è¯¦ç»†åˆ†æ")
                
                # ä¸ºæ¯ä¸ªæ ¸å¿ƒç ”ç©¶é—®é¢˜ç”Ÿæˆæ·±åº¦åˆ†ææ–‡æœ¬
                detailed_analyses = {}
                
                # ç”Ÿæˆæ¯ä¸ªæ ¸å¿ƒé—®é¢˜çš„è¯¦ç»†åˆ†æ
                for question_data in core_questions:
                    question_title = question_data['title']
                    question_id = question_data.get('step_number', 0)
                    
                    # æ”¶é›†è¯¥é—®é¢˜çš„æ‰€æœ‰å‘ç°
                    all_findings = question_data.get('findings', [])
                    
                    # æ”¶é›†è¯¥é—®é¢˜çš„æ‰€æœ‰æŸ¥è¯¢ç»“æœ
                    query_results = question_data.get('query_results', [])
                    
                    # åˆ›å»ºæ¯ä¸ªé—®é¢˜çš„è¯¦ç»†åˆ†æ
                    detailed_analysis = f"## ç ”ç©¶é—®é¢˜{question_id}: {question_title}\n\n"
                    detailed_analysis += f"**é—®é¢˜æè¿°**: {question_data['description']}\n\n"
                    
                    # æ·»åŠ æœç´¢è¿‡ç¨‹
                    if query_results:
                        detailed_analysis += f"### æœç´¢è¿‡ç¨‹\n\n"
                        detailed_analysis += f"ä¸ºå›ç­”è¯¥é—®é¢˜ï¼Œæˆ‘ä»¬æ‰§è¡Œäº†ä»¥ä¸‹{len(question_data.get('search_queries', []))}ä¸ªæœç´¢æŸ¥è¯¢ï¼š\n\n"
                        
                        # æ·»åŠ æ¯ä¸ªæŸ¥è¯¢çš„ç»“æœå’Œå°ç»“
                        for idx, query_result in enumerate(query_results):
                            query = query_result.get('query', '')
                            findings = query_result.get('findings', [])
                            summary = query_result.get('summary', '')
                            
                            detailed_analysis += f"#### æŸ¥è¯¢ {idx+1}: `{query}`\n\n"
                            
                            # æ·»åŠ è¯¥æŸ¥è¯¢çš„å°ç»“
                            if summary:
                                detailed_analysis += f"**å°ç»“**: {summary}\n\n"
                            
                            # æ·»åŠ å…³é”®å‘ç°
                            if findings:
                                detailed_analysis += f"**å…³é”®å‘ç°**:\n\n"
                                for finding in findings[:3]:  # é™åˆ¶æ¯ä¸ªæŸ¥è¯¢æ˜¾ç¤ºå‰3ä¸ªå‘ç°
                                    detailed_analysis += f"- {finding}\n"
                                detailed_analysis += "\n"
                    
                    # æ·»åŠ è¯¥é—®é¢˜çš„æœ€ç»ˆåˆ†æ
                    if question_data.get('analysis'):
                        detailed_analysis += f"### é—®é¢˜åˆ†æ\n\n{question_data['analysis']}\n\n"
                    
                    # å­˜å‚¨è¯¥é—®é¢˜çš„è¯¦ç»†åˆ†æ
                    detailed_analyses[question_title] = detailed_analysis
                    
                # å‡†å¤‡ç ”ç©¶æŠ¥å‘Šçš„æ•°æ®
                findings_text += f"# ç ”ç©¶åˆ†ææŠ¥å‘Š: {process.topic}\n\n"
                findings_text += f"*æœ¬æŠ¥å‘Šæ·±å…¥ç ”ç©¶äº†ä¸»é¢˜: {process.topic}*\n\n"
                
                if process.requirements:
                    findings_text += f"**ç ”ç©¶è¦æ±‚**: {process.requirements}\n\n"
                
                # æ·»åŠ æ•°æ®æ”¶é›†ç»Ÿè®¡
                total_search_queries = sum(len(step.get('search_queries', [])) for step in process.research_steps)
                total_search_results = sum(len(step.get('search_results', [])) for step in process.research_steps)
                total_findings = sum(len(step.get('findings', [])) for step in process.research_steps)
                
                findings_text += f"## ç ”ç©¶æ–¹æ³•ä¸æ•°æ®\n\n"
                findings_text += f"- ç ”ç©¶æ·±åº¦: æ‰§è¡Œäº† {len(core_questions)} ä¸ªæ ¸å¿ƒç ”ç©¶é—®é¢˜çš„è¯¦ç»†åˆ†æ\n"
                findings_text += f"- æœç´¢èŒƒå›´: æ‰§è¡Œäº† {total_search_queries} ä¸ªç²¾å¿ƒè®¾è®¡çš„æœç´¢æŸ¥è¯¢\n"
                findings_text += f"- æ•°æ®é‡: åˆ†æäº† {total_search_results} ä¸ªæœç´¢ç»“æœï¼Œæå–äº† {total_findings} æ¡ç›¸å…³å‘ç°\n\n"
                
                # æ·»åŠ è¯¦ç»†çš„æ¯ä¸ªç ”ç©¶é—®é¢˜åˆ†æ
                findings_text += "## ç ”ç©¶é—®é¢˜åˆ—è¡¨\n\n"
                
                # æ·»åŠ é—®é¢˜åˆ—è¡¨
                for idx, question_data in enumerate(core_questions):
                    findings_text += f"{idx+1}. **{question_data['title']}**\n"
                
                findings_text += "\n## è¯¦ç»†åˆ†æ\n\n"
                
                # æ·»åŠ æ¯ä¸ªé—®é¢˜çš„è¯¦ç»†åˆ†æ
                for question_data in core_questions:
                    findings_text += detailed_analyses[question_data['title']]
                    if step_data.get('search_results'):
                        findings_text += "#### æ•°æ®æ¥æº\n\n"
                        
                        # æ·»åŠ æ•°æ®æ¥æºæ±‡æ€»è¡¨æ ¼
                        findings_text += "| æ¥æº | æ ‡é¢˜ | ç±»å‹ |\n"
                        findings_text += "|-------|------|---------|\n"
                        
                        # å¢åŠ æ˜¾ç¤ºæ•°é‡ï¼Œç¡®ä¿ä¸ä¸¢å¤±ä¿¡æ¯
                        for result in step_data['search_results'][:10]:  # æ˜¾ç¤ºå‰10ä¸ªç»“æœ
                            result_type = result.get('type', 'ç½‘é¡µ')
                            findings_text += f"| {result['name']} | {result['title']} | {result_type} |\n"
                        
                        # å¦‚æœè¿˜æœ‰æ›´å¤šç»“æœï¼Œæ·»åŠ æ±‡æ€»ä¿¡æ¯
                        if len(step_data['search_results']) > 10:
                            remaining = len(step_data['search_results']) - 10
                            findings_text += f"\n*åŠå…¶ä»– {remaining} ä¸ªæ•°æ®æ¥æº*\n"
                            
                        findings_text += "\n"
                    
                    # æ·»åŠ ç ”ç©¶å‘ç° - å¯¹å‘ç°è¿›è¡Œåˆ†ç±»å’Œæ ‡æ³¨
                    if step_data.get('findings'):
                        findings_text += "#### è¯¦ç»†å‘ç°\n\n"
                        
                        # å°è¯•æ£€æµ‹å‘ç°ä¸­çš„æ•°æ®ç±»å‹ï¼ŒåŠ ä»¥æ ‡è®°
                        statistical_findings = []
                        trend_findings = []
                        comparison_findings = []
                        general_findings = []
                        
                        for finding in step_data['findings']:
                            # æ£€æµ‹åŒ…å«æ•°å­—ç»Ÿè®¡ä¿¡æ¯çš„å‘ç°
                            if re.search(r'\d+(\.\d+)?\s*(%|ç™¾åˆ†æ¯”|äº¿|ä¸‡|åƒ|å…ƒ)', finding):
                                statistical_findings.append(finding)
                            # æ£€æµ‹è¶‹åŠ¿ç›¸å…³çš„å‘ç°
                            elif re.search(r'(å¢é•¿|ä¸‹é™|è¶‹åŠ¿|å˜åŒ–|å‘å±•|æœªæ¥|\d{4}å¹´)', finding):
                                trend_findings.append(finding)
                            # æ£€æµ‹æ¯”è¾ƒä¿¡æ¯
                            elif re.search(r'(æ¯”|ç›¸å¯¹|ä¸|å¯¹æ¯”|ä¸åŒ|ç±»ä¼¼|å·®å¼‚)', finding):
                                comparison_findings.append(finding)
                            # å…¶ä»–é€šç”¨å‘ç°
                            else:
                                general_findings.append(finding)
                                
                        # æŒ‰ç±»åˆ«æ·»åŠ å‘ç°
                        if statistical_findings:
                            findings_text += "**ç»Ÿè®¡æ•°æ®:**\n\n"
                            for finding in statistical_findings:
                                findings_text += f"- {finding}\n"
                            findings_text += "\n"
                            
                        if trend_findings:
                            findings_text += "**è¶‹åŠ¿åˆ†æ:**\n\n"
                            for finding in trend_findings:
                                findings_text += f"- {finding}\n"
                            findings_text += "\n"
                            
                        if comparison_findings:
                            findings_text += "**æ¯”è¾ƒåˆ†æ:**\n\n"
                            for finding in comparison_findings:
                                findings_text += f"- {finding}\n"
                            findings_text += "\n"
                            
                        if general_findings:
                            findings_text += "**å…¶ä»–å‘ç°:**\n\n"
                            for finding in general_findings:
                                findings_text += f"- {finding}\n"
                            findings_text += "\n"
                    
                    # æ·»åŠ åˆ†æç»“æœ - ä½¿ç”¨å¼•ç”¨æ ¼å¼å¢å¼ºå¯è¯»æ€§
                    if step_data.get('analysis'):
                        findings_text += "#### ä¸“å®¶åˆ†æ\n\n"
                        # ä½¿ç”¨å¼•ç”¨æ ¼å¼ç¾åŒ–åˆ†ææ˜¾ç¤º
                        analysis_lines = step_data['analysis'].strip().split('\n')
                        for line in analysis_lines:
                            if line.strip():
                                # å¯¹æ ‡é¢˜è¡Œä½¿ç”¨åŠ ç²—
                                if re.match(r'^[\s]*\d+\.\s+', line) or line.strip().endswith(':'):
                                    findings_text += f"**{line.strip()}**\n\n"
                                else:
                                    findings_text += f"> {line.strip()}\n\n"
                    
                    findings_text += "---\n\n"
                
                # ç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š
                logger.info("å¼€å§‹ç”Ÿæˆç ”ç©¶æŠ¥å‘Š")
                try:
                    # æå–æ‰€æœ‰ç ”ç©¶å‘ç°ç”¨äºç”ŸæˆæŠ¥å‘Š
                    all_findings = []
                    for step in process.research_steps:
                        if step.get('findings'):
                            all_findings.extend(step.get('findings', []))
                    
                    # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„ç ”ç©¶å‘ç°ï¼Œæ·»åŠ å¤‡ç”¨å†…å®¹
                    if len(all_findings) < 3:
                        # æ·»åŠ ä»findings_textä¸­æå–çš„å†…å®¹
                        extracted_findings = [line.strip('- ') for line in findings_text.split('\n') 
                                            if line.strip().startswith('- ') and len(line) > 5]
                        all_findings.extend(extracted_findings)
                    
                    # ç¡®ä¿ç ”ç©¶å‘ç°ä¸é‡å¤
                    unique_findings = list(dict.fromkeys(all_findings))
                    
                    # è®°å½•ç ”ç©¶å‘ç°æ•°é‡
                    logger.info(f"ä¸ºç ”ç©¶æŠ¥å‘Šå‡†å¤‡äº† {len(unique_findings)} æ¡ç ”ç©¶å‘ç°")
                    
                    # å°è¯•é€šè¿‡AIæœåŠ¡ç”Ÿæˆç ”ç©¶æŠ¥å‘Šï¼Œä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å’Œå‚æ•°
                    process.report = process.ai_service.analyze_research_report(
                        unique_findings,
                        process.topic,
                        process.requirements
                    )
                    logger.info("ç ”ç©¶æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
                except Exception as e:
                    # å¦‚æœAIæœåŠ¡å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆç”ŸæˆæŠ¥å‘Š
                    logger.warning(f"ä½¿ç”¨AIæœåŠ¡ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}ï¼Œåˆ‡æ¢åˆ°å¤‡ç”¨æ–¹æ¡ˆ")
                    
                    # åˆ›å»ºä¸€ä¸ªç»“æ„åŒ–çš„æŠ¥å‘Šï¼Œä½¿ç”¨å·²ç»æå–çš„ç ”ç©¶å‘ç°
                    process.report = self._generate_fallback_report(
                        process.topic, 
                        process.requirements,
                        unique_findings, 
                        process.research_steps
                    )
                    logger.info("ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆç”ŸæˆæŠ¥å‘ŠæˆåŠŸ")
                
                # å®Œæˆç ”ç©¶
                process.progress = 100
                process.status = "completed"
                process.current_step = "ç ”ç©¶å®Œæˆ"
            except Exception as e:
                logger.error(f"ç”Ÿæˆç ”ç©¶æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
                process.error = f"ç”Ÿæˆç ”ç©¶æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}"
                process.status = "error"
            
        except Exception as e:
            process.error = f"ç ”ç©¶è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
            process.status = "error"
    
    def _generate_fallback_report(self, topic, requirements, findings, research_steps):
        """ç”Ÿæˆå¤‡ç”¨ç ”ç©¶æŠ¥å‘Šï¼Œå½“AIæœåŠ¡æ— æ³•ç”ŸæˆæŠ¥å‘Šæ—¶ä½¿ç”¨
        
        Args:
            topic: ç ”ç©¶ä¸»é¢˜
            requirements: ç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚
            findings: å·²ç»æå–çš„ç ”ç©¶å‘ç°åˆ—è¡¨
            research_steps: ç ”ç©¶æ­¥éª¤æ•°æ®
        """
        from datetime import datetime
        import re
        
        logger.info(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆä¸ºä¸»é¢˜ '{topic}' ç”Ÿæˆç ”ç©¶æŠ¥å‘Š")
        
        # æŒ‰ç±»å‹åˆ†ç±»ç ”ç©¶å‘ç°
        statistical_findings = []
        trend_findings = []
        comparison_findings = []
        general_findings = []
        
        for finding in findings:
            # æ£€æµ‹åŒ…å«æ•°å­—ç»Ÿè®¡ä¿¡æ¯çš„å‘ç°
            if re.search(r'\d+(\.\d+)?\s*(%|ç™¾åˆ†æ¯”|äº¿|ä¸‡|åƒ|å…ƒ)', finding):
                statistical_findings.append(finding)
            # æ£€æµ‹è¶‹åŠ¿ç›¸å…³çš„å‘ç°
            elif re.search(r'(å¢é•¿|ä¸‹é™|è¶‹åŠ¿|å˜åŒ–|å‘å±•|æœªæ¥|\d{4}å¹´)', finding):
                trend_findings.append(finding)
            # æ£€æµ‹æ¯”è¾ƒä¿¡æ¯
            elif re.search(r'(æ¯”|ç›¸å¯¹|ä¸|å¯¹æ¯”|ä¸åŒ|ç±»ä¼¼|å·®å¼‚)', finding):
                comparison_findings.append(finding)
            # å…¶ä»–é€šç”¨å‘ç°
            else:
                general_findings.append(finding)
        
        # æå–å®Œæˆçš„ç ”ç©¶æ­¥éª¤æ ‡é¢˜
        completed_steps = [step.get('title', '') for step in research_steps if step.get('completed', False)]
        
        # ç”Ÿæˆå½“å‰æ—¶é—´
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # æ„å»ºæŠ¥å‘Šæ ‡é¢˜
        report = f"""# {topic} ç ”ç©¶æŠ¥å‘Š

## æ‘˜è¦

æœ¬æŠ¥å‘Šæ±‡æ€»äº†å…³äº"{topic}"çš„ç»¼åˆç ”ç©¶å‘ç°ï¼Œå¹¶æä¾›äº†ç›¸å…³å¸‚åœºåˆ†æå’Œå»ºè®®ã€‚ç ”ç©¶é€šè¿‡å¤šä¸ªæ•°æ®æºå’Œç½‘ç»œèµ„æºè¿›è¡Œï¼Œæ¶µç›–äº†å¤šä¸ªç›¸å…³æ–¹é¢ã€‚
"""
        
        # æ·»åŠ ç”¨æˆ·éœ€æ±‚éƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
        if requirements:
            report += f"""
## ç ”ç©¶ç›®æ ‡ä¸èŒƒå›´

å½“å‰ç ”ç©¶æ˜¯åº”ç”¨æˆ·è¦æ±‚è¿›è¡Œçš„ï¼Œå…·ä½“éœ€æ±‚å¦‚ä¸‹ï¼š

"{requirements}"

ç ”ç©¶å†…å®¹æ¶µç›–äº†ä»¥ä¸‹ä¸»è¦æ–¹é¢ï¼š

{', '.join(completed_steps)}
"""
        
        # æ·»åŠ ç»Ÿè®¡æ•°æ®å‘ç°
        if statistical_findings:
            report += """
## å…³é”®æ•°æ®ä¸ç»Ÿè®¡

ä»¥ä¸‹æ˜¯ç ”ç©¶ä¸­å‘ç°çš„é‡è¦æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯ï¼š

"""
            for finding in statistical_findings:
                report += f"- {finding}\n"
        
        # æ·»åŠ è¶‹åŠ¿ç›¸å…³å‘ç°
        if trend_findings:
            report += """
## å¸‚åœºè¶‹åŠ¿åˆ†æ

ç ”ç©¶å‘ç°äº†ä»¥ä¸‹å¸‚åœºå’Œè¡Œä¸šè¶‹åŠ¿ï¼š

"""
            for finding in trend_findings:
                report += f"- {finding}\n"
        
        # æ·»åŠ æ¯”è¾ƒåˆ†æå‘ç°
        if comparison_findings:
            report += """
## æ¯”è¾ƒåˆ†æ

ä»¥ä¸‹æ˜¯ä¸åŒæ–¹é¢çš„æ¯”è¾ƒåˆ†æï¼š

"""
            for finding in comparison_findings:
                report += f"- {finding}\n"
        
        # æ·»åŠ ä¸€èˆ¬å‘ç°
        if general_findings:
            report += """
## å…¶ä»–é‡è¦å‘ç°

ç ”ç©¶è¿‡ç¨‹ä¸­æ”¶é›†çš„å…¶ä»–é‡è¦å‘ç°åŒ…æ‹¬ï¼š

"""
            for finding in general_findings:
                report += f"- {finding}\n"
        
        # æ·»åŠ ç»“è®ºå’Œå»ºè®®
        report += f"""
## ç»“è®ºä¸å»ºè®®

åŸºäºä»¥ä¸Šç ”ç©¶å‘ç°ï¼Œæˆ‘ä»¬å¯¹"{topic}"æå‡ºä»¥ä¸‹ç»“è®ºå’Œå»ºè®®ï¼š

1. æœ¬ç ”ç©¶æä¾›äº†å¤šè§’åº¦çš„è§‚ç‚¹å’Œæ•°æ®ï¼Œåº”æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯è¿›è¡Œé€‰æ‹©åº”ç”¨ã€‚
2. å»ºè®®è¿›ä¸€æ­¥æ·±å…¥åˆ†æç‰¹å®šé¢†åŸŸçš„æ•°æ®ï¼Œä»¥è·å¾—æ›´ç²¾ç¡®çš„æ´å¯Ÿã€‚
3. å¸‚åœºä¸æŠ€æœ¯ç¯å¢ƒæŒç»­å˜åŒ–ï¼Œå®šæœŸè¿›è¡Œç±»ä¼¼ç ”ç©¶å¯ä»¥è·Ÿè¸ªæœ€æ–°è¶‹åŠ¿ã€‚

---
æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {current_time}
"""
        
        logger.info("é«˜è´¨é‡å¤‡ç”¨æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        return report
        
    def start_research_execution(self, process_id):
        """å¼€å§‹æ‰§è¡Œç ”ç©¶ï¼ˆåœ¨ç”¨æˆ·ç¡®è®¤è®¡åˆ’åï¼‰"""
        process = self.get_research_process(process_id)
        if not process:
            logger.error(f"æ— æ³•æ‰¾åˆ°ç ”ç©¶è¿›ç¨‹: {process_id}")
            return False
            
        if process.status != "waiting_confirmation":
            logger.error(f"ç ”ç©¶è¿›ç¨‹çŠ¶æ€ä¸æ­£ç¡®ï¼Œæ— æ³•æ‰§è¡Œ: {process.status}")
            return False
            
        # æ›´æ–°çŠ¶æ€ï¼Œè¡¨æ˜ç”¨æˆ·å·²ç¡®è®¤ç ”ç©¶è®¡åˆ’
        process.status = "confirmed"
        logger.info(f"ç ”ç©¶è®¡åˆ’å·²è¢«ç”¨æˆ·ç¡®è®¤: {process_id}")
        
        # å¯åŠ¨ä¸€ä¸ªæ–°çº¿ç¨‹æ¥æ‰§è¡Œç ”ç©¶
        logger.info(f"å¼€å§‹æ‰§è¡Œç ”ç©¶è¿‡ç¨‹: {process_id}")
        thread = threading.Thread(target=self._conduct_research, args=(process,))
        thread.daemon = True
        thread.start()
        return True
    
    def _parse_research_plan(self, plan, prioritize_questions=False):
        """ä»ç ”ç©¶è®¡åˆ’ä¸­è§£æå‡ºå„ä¸ªç ”ç©¶æ­¥éª¤ - å¢å¼ºç‰ˆæœ¬
        
        Args:
            plan: ç ”ç©¶è®¡åˆ’æ–‡æœ¬
            prioritize_questions: æ˜¯å¦ä¼˜å…ˆå¤„ç†ç ”ç©¶é—®é¢˜ï¼ˆæ¯”å¦‚å°†ç ”ç©¶é—®é¢˜æ”¾åœ¨å‰é¢å¤„ç†ï¼‰
            
        Returns:
            è§£æå‡ºçš„ç ”ç©¶æ­¥éª¤åˆ—è¡¨
        """
        try:
            # å¦‚æœè®¡åˆ’ä¸ºç©ºæˆ–ä¸åˆæ³•ï¼Œè¿”å›é»˜è®¤çš„ç ”ç©¶æ­¥éª¤
            if not plan:
                logger.warning("ç ”ç©¶è®¡åˆ’ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ­¥éª¤")
                return self._get_default_research_steps()
            
            logger.info("-------- å¼€å§‹è§£æç ”ç©¶è®¡åˆ’ --------")
            logger.info(f"åŸå§‹è®¡åˆ’\n{plan}")
            
            # æ–¹æ³•1ï¼šä½¿ç”¨æ ‡é¢˜æ¨¡å¼è¯†åˆ«æ­¥éª¤
            steps = self._parse_steps_by_headers(plan)
            
            # æ–¹æ³•2ï¼šå¦‚æœæ ‡é¢˜è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç¼–å·åˆ—è¡¨åŒ¹é…
            if not steps:
                logger.info("æ ‡é¢˜è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç¼–å·åˆ—è¡¨åŒ¹é…")
                steps = self._parse_steps_by_numbered_list(plan)
            
            # æ–¹æ³•3ï¼šå¦‚æœä¸Šè¿°æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç©ºè¡Œåˆ†éš”æ®µè½
            if not steps:
                logger.info("ç¼–å·è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ®µè½åˆ†æ")
                steps = self._parse_steps_by_paragraphs(plan)
            
            # æ ‡è®°ç ”ç©¶é—®é¢˜å’ŒçŸ¥è¯†æ€§æ­¥éª¤
            research_questions = []
            knowledge_steps = []
            analysis_steps = []
            
            # åˆ†ç±»æ¯ä¸ªæ­¥éª¤
            for i, step in enumerate(steps):
                step_title = step["title"]
                
                # æ ‡è®°æ˜¯å¦ä¸ºæ ¸å¿ƒç ”ç©¶é—®é¢˜
                if ("æ ¸å¿ƒ" in step_title and "é—®é¢˜" in step_title) or \
                   ("ç ”ç©¶é—®é¢˜" in step_title) or \
                   ("å…³é”®é—®é¢˜" in step_title) or \
                   any(q in step_title.lower() for q in ["é—®é¢˜", "question", "inquiry", "æ¢ç©¶"]):
                    step["is_core_question"] = True
                    research_questions.append(step)
                # æ ‡è®°çŸ¥è¯†æ€§æ­¥éª¤
                elif ("ç ”ç©¶ç›®æ ‡" in step_title) or ("ç ”ç©¶æ–¹æ³•" in step_title) or \
                     ("ç ”ç©¶èƒŒæ™¯" in step_title) or ("ç ”ç©¶èŒƒå›´" in step_title):
                    step["is_knowledge_step"] = True
                    knowledge_steps.append(step)
                # å…¶ä»–ä¸ºåˆ†ææ­¥éª¤
                else:
                    analysis_steps.append(step)
                    
                # ä¿å­˜åŸå§‹æ–‡æœ¬ç‰‡æ®µï¼Œç”¨äºå‰ç«¯æ˜¾ç¤º
                step["original_content"] = step["description"]
                
                # ç¡®ä¿æ­¥éª¤æ ‡é¢˜åˆç†
                if len(step["title"]) > 50:
                    step["title"] = step["title"][:47] + "..."
                    
                # ç¡®ä¿æ­¥éª¤æè¿°åˆé€‚
                if len(step["description"]) > 500:
                    step["description"] = step["description"][:150] + "..." + step["description"][-150:]
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªç ”ç©¶æ­¥éª¤
            if not steps:
                logger.warning("æ‰€æœ‰è§£ææ–¹æ³•å‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ­¥éª¤")
                return self._get_default_research_steps()
            
            # å¦‚æœéœ€è¦ä¼˜å…ˆå¤„ç†ç ”ç©¶é—®é¢˜ï¼Œè°ƒæ•´æ­¥éª¤é¡ºåº
            if prioritize_questions and research_questions:
                logger.info(f"å¯ç”¨ä¼˜å…ˆå¤„ç†ç ”ç©¶é—®é¢˜æ¨¡å¼ï¼Œæ‰¾åˆ° {len(research_questions)} ä¸ªç ”ç©¶é—®é¢˜")
                # æŒ‰ç­›é€‰å½’ç±»é‡æ’æ­¥éª¤ï¼šå…ˆçŸ¥è¯†æ€§æ­¥éª¤ã€ç„¶åæ ¸å¿ƒç ”ç©¶é—®é¢˜ã€æœ€åæ˜¯åˆ†ææ­¥éª¤
                reordered_steps = knowledge_steps + research_questions + analysis_steps
                steps = reordered_steps
                logger.info(f"é‡æ’å®Œæˆï¼Œæ–°çš„æ­¥éª¤é¡ºåº: çŸ¥è¯†æ€§æ­¥éª¤({len(knowledge_steps)}) -> ç ”ç©¶é—®é¢˜({len(research_questions)}) -> åˆ†ææ­¥éª¤({len(analysis_steps)})")
            
            # è®°å½•æå–çš„æ­¥éª¤
            logger.info(f"æˆåŠŸæå–äº† {len(steps)} ä¸ªç ”ç©¶æ­¥éª¤:")
            for i, step in enumerate(steps):
                step_type = 'æ ¸å¿ƒç ”ç©¶é—®é¢˜' if step.get('is_core_question', False) else 'çŸ¥è¯†æ€§æ­¥éª¤' if step.get('is_knowledge_step', False) else 'åˆ†ææ­¥éª¤'
                logger.info(f"  æ­¥éª¤ {i+1}: {step['title']} ({step_type})")
                logger.info(f"  æè¿°: {step['description'][:100]}...")
            
            return steps
        
        except Exception as e:
            logger.error(f"è§£æç ”ç©¶è®¡åˆ’æ—¶å‡ºé”™: {str(e)}")
            logger.exception(e)  # æ‰“å°å®Œæ•´å †æ ˆè·Ÿè¸ª
            # å‡ºé”™æ—¶è¿”å›é»˜è®¤æ­¥éª¤
            return self._get_default_research_steps()
    
    def _get_default_research_steps(self):
        """è·å–é»˜è®¤çš„ç ”ç©¶æ­¥éª¤"""
        return [
            {"title": "å¸‚åœºæ¦‚è§ˆ", "description": "åˆ†æå¸‚åœºè§„æ¨¡å’Œå¢é•¿è¶‹åŠ¿", "original_content": "åˆ†æå¸‚åœºè§„æ¨¡å’Œå¢é•¿è¶‹åŠ¿"}, 
            {"title": "ç«äº‰åˆ†æ", "description": "è¯„ä¼°ä¸»è¦ç«äº‰å¯¹æ‰‹å’Œç«äº‰æ€åŠ¿", "original_content": "è¯„ä¼°ä¸»è¦ç«äº‰å¯¹æ‰‹å’Œç«äº‰æ€åŠ¿"}, 
            {"title": "å…³é”®é©±åŠ¨å› ç´ ", "description": "è¯„ä¼°å¸‚åœºæˆé•¿çš„ä¸»è¦é©±åŠ¨å› ç´ ", "original_content": "è¯„ä¼°å¸‚åœºæˆé•¿çš„ä¸»è¦é©±åŠ¨å› ç´ "},
            {"title": "è¶‹åŠ¿å±•æœ›", "description": "åˆ†ææœªæ¥å‘å±•è¶‹åŠ¿å’Œæœºä¼š", "original_content": "åˆ†ææœªæ¥å‘å±•è¶‹åŠ¿å’Œæœºä¼š"}
        ]
        
    def _parse_steps_by_headers(self, plan):
        """æ ¹æ®æ ‡é¢˜æ ‡è®°(#ã€##ç­‰)è§£æç ”ç©¶æ­¥éª¤"""
        steps = []
        current_step = None
        
        for line in plan.split('\n'):
            line = line.strip()
            
            # æ£€æµ‹æ ‡é¢˜è¡Œ
            header_match = re.match(r'^(#+)\s+(.+)$', line)
            if header_match:
                header_level = len(header_match.group(1))
                title = header_match.group(2).strip()
                
                # å¦‚æœæ ‡é¢˜çº§åˆ«ä¸º1æˆ–30ä¸Šçš„å­—ç¬¦ï¼Œåˆ™å¯èƒ½æ˜¯ä¸»æ ‡é¢˜ï¼Œä¸æ˜¯ç ”ç©¶æ­¥éª¤
                if (header_level > 1 or len(title) <= 30) and not title.lower().startswith(('overview', 'æ¦‚è¿°', 'ç®€ä»‹', 'ä»‹ç»', 'å…³äº')):
                    # ä¿å­˜å½“å‰æ­¥éª¤ï¼ˆå¦‚æœæœ‰ï¼‰
                    if current_step and current_step["title"] and current_step["description"].strip():
                        steps.append(current_step)
                    
                    # åˆ›å»ºæ–°æ­¥éª¤
                    current_step = {"title": title, "description": ""}
                elif current_step:  # å¦‚æœä¸ºå­æ ‡é¢˜æˆ–å…¶ä»–æ ‡é¢˜ç±»å‹ï¼Œå¹¶ä¸”å·²æœ‰å½“å‰æ­¥éª¤
                    current_step["description"] += line + "\n"
            
            # æ”¶é›†éæ ‡é¢˜è¡Œ
            elif current_step and line:
                current_step["description"] += line + "\n"
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ­¥éª¤
        if current_step and current_step["title"] and current_step["description"].strip():
            steps.append(current_step)
            
        return steps
    
    def _parse_steps_by_numbered_list(self, plan):
        """æ ¹æ®ç¼–å·åˆ—è¡¨(1. 2. ç­‰)è§£æç ”ç©¶æ­¥éª¤"""
        steps = []
        lines = plan.split('\n')
        i = 0
        
        while i < len(lines):
            line = lines[i].strip()
            
            # åŒ¹é…ç¼–å·å½¢å¼ï¼š1. 2. æˆ– 1) 2) ç­‰
            num_match = re.match(r'^(\d+)[.)]\s+(.+)$', line)
            if num_match:
                number = int(num_match.group(1))
                title = num_match.group(2).strip()
                
                # å¦‚æœæ˜¯ç¼–å·ä¸º1çš„æ–°åˆ—è¡¨æˆ–ç¼–å·æŒ‰é¡ºåºçš„æ¡ç›®
                if number == 1 or len(steps) > 0:
                    description = ""
                    j = i + 1
                    
                    # æŸ¥æ‰¾å½“å‰æ¡ç›®å…³è”çš„æ‰€æœ‰æ–‡æœ¬
                    while j < len(lines) and not re.match(r'^\d+[.)]\s+', lines[j].strip()):
                        if lines[j].strip():  # åªæ·»åŠ éç©ºè¡Œ
                            description += lines[j].strip() + "\n"
                        j += 1
                        if j >= len(lines) or lines[j].strip() == "":
                            break
                    
                    steps.append({"title": title, "description": description})
                    i = j - 1  # è°ƒæ•´ç´¢å¼•ä½ç½®
            i += 1
            
        return steps
    
    def _parse_steps_by_paragraphs(self, plan):
        """æ ¹æ®æ®µè½ç»“æ„è§£æç ”ç©¶æ­¥éª¤"""
        steps = []
        paragraphs = re.split(r'\n\s*\n', plan)  # æŒ‰ç©ºè¡Œåˆ†éš”æ®µè½
        
        # è¿‡æ»¤æ‰å¯èƒ½çš„å¼•è¨€æ®µè½
        filtered_paragraphs = []
        for p in paragraphs:
            p = p.strip()
            # è·³è¿‡ç©ºæ®µè½å’Œå¯èƒ½æ˜¯æ ‡é¢˜çš„æ®µè½
            if not p or len(p.split('\n')) == 1 and len(p) < 40:
                continue
            filtered_paragraphs.append(p)
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ®µè½ï¼Œå†è¿›ä¸€æ­¥å°è¯•åˆ†å‰²
        if len(filtered_paragraphs) == 1:
            text = filtered_paragraphs[0]
            # å°è¯•æŒ‰å¯èƒ½çš„åˆ†éš”ç¬¦åˆ†å‰²
            split_paragraphs = re.split(r'\n\s*[-*]\s+|\n\s*\d+\.\s+', text)
            if len(split_paragraphs) > 1:
                filtered_paragraphs = split_paragraphs
        
        # é™åˆ¶æœ€å¤š6ä¸ªæ­¥éª¤
        max_steps = min(len(filtered_paragraphs), 6)
        
        for i in range(max_steps):
            p = filtered_paragraphs[i].strip()
            lines = p.split('\n')
            
            # å–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜ï¼Œå¦‚æœå®ƒè¶³å¤ŸçŸ­
            if len(lines) > 0 and len(lines[0]) < 100:
                title = lines[0].strip()
                description = '\n'.join(lines[1:]).strip() if len(lines) > 1 else ""
            else:
                # å¦‚æœç¬¬ä¸€è¡Œå¤ªé•¿ï¼Œç”Ÿæˆä¸€ä¸ªåˆé€‚çš„æ ‡é¢˜
                words = re.findall(r'\w+', p[:200])  # å–å‰200ä¸ªå­—ç¬¦åˆ†æ
                title = f"ç ”ç©¶æ­¥éª¤ {i+1}: " + (' '.join(words[:5]) + "..." if len(words) > 5 else ' '.join(words))
                description = p
            
            steps.append({"title": title, "description": description})
            
        return steps
            
    def _generate_query_summary(self, query, findings, question_title):
        """ä¸ºå•ä¸ªæŸ¥è¯¢ç”Ÿæˆå°ç»“ï¼Œä¾¿äºç”¨æˆ·ç†è§£è¯¥æŸ¥è¯¢çš„ç ”ç©¶æˆæœ
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            findings: ä»è¯¥æŸ¥è¯¢ä¸­æå–çš„å‘ç°åˆ—è¡¨
            question_title: ç ”ç©¶é—®é¢˜æ ‡é¢˜
            
        Returns:
            str: è¯¥æŸ¥è¯¢çš„å°ç»“åˆ†æ
        """
        try:
            if not findings:
                return f"æœªä»æŸ¥è¯¢'{query}'ä¸­è·å–åˆ°æœ‰æ•ˆä¿¡æ¯"
                
            # ä½¿ç”¨AIæœåŠ¡ç”Ÿæˆå°ç»“
            from app.services.siliconflow_service import siliconflow_service
            
            # æ„å»ºæç¤º
            prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä»æœç´¢æŸ¥è¯¢'{query}'ä¸­æå–çš„ä¿¡æ¯ï¼Œä¸ºç ”ç©¶é—®é¢˜"{question_title}"ç”Ÿæˆä¸€ä¸ªç®€çŸ­ä½†æœ‰ä¿¡æ¯é‡çš„å°ç»“ã€‚
            
æå–çš„ä¿¡æ¯å¦‚ä¸‹ï¼š
{chr(10).join([f'- {finding}' for finding in findings])}

è¯·æ€»ç»“ä¸Šè¿°ä¿¡æ¯çš„å…³é”®ç‚¹ï¼Œç¡®ä¿å°ç»“ï¼š
1. ç›´æ¥å›ç­”ä¸æŸ¥è¯¢'{query}'ç›¸å…³çš„éƒ¨åˆ†
2. æä¾›å…·ä½“çš„æ•°æ®ç‚¹å’Œæ´è§ï¼ˆå¦‚æœæœ‰ï¼‰
3. ä¸è¶…è¿‡150å­—
4. ä¸è¦æ·»åŠ ä»»ä½•æœªåœ¨æä¾›çš„ä¿¡æ¯ä¸­å‡ºç°çš„å†…å®¹

å°ç»“ï¼š"""
            
            # è°ƒç”¨APIç”Ÿæˆå°ç»“
            payload = {
                "model": "Pro/deepseek-ai/DeepSeek-V3",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿ç®€æ˜æ‰¼è¦åœ°æ€»ç»“æŸ¥è¯¢ç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 300
            }
            
            response = siliconflow_service._make_api_request("chat/completions", payload)
            
            if response and "choices" in response and len(response["choices"]) > 0:
                summary = response["choices"][0]["message"]["content"].strip()
                logger.info(f"ä¸ºæŸ¥è¯¢'{query}'ç”Ÿæˆäº†å°ç»“ï¼š{summary[:50]}...")
                return summary
            else:
                # å¦‚æœAIæœåŠ¡å¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªç®€å•çš„å°ç»“
                return self._generate_simple_summary(findings)
                
        except Exception as e:
            logger.error(f"ä¸ºæŸ¥è¯¢ç”Ÿæˆå°ç»“æ—¶å‡ºé”™: {str(e)}")
            return self._generate_simple_summary(findings)
    
    def _generate_simple_summary(self, findings):
        """ç”Ÿæˆä¸€ä¸ªç®€å•çš„å°ç»“ï¼Œå½“AIæœåŠ¡å¤±è´¥æ—¶ä½¿ç”¨"""
        if not findings:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            
        # ç®€å•åœ°åˆå¹¶å‰ä¸¤ä¸ªå‘ç°
        summary = findings[0]
        if len(findings) > 1:
            summary += "\næ­¤å¤–ï¼Œ" + findings[1]
            
        return summary
    
    def _generate_step_search_queries(self, step_title, step_description, topic):
        """ä¸ºç ”ç©¶æ­¥éª¤ç”Ÿæˆæœç´¢æŸ¥è¯¢ - æ”¹è¿›ç‰ˆæœ¬ï¼Œç¡®ä¿æŸ¥è¯¢ä¸­åŒ…å«ä¸»é¢˜ç›¸å…³çš„å…·ä½“å…³é”®è¯"""
        try:
            # å¦‚æœæ˜¯é»˜è®¤æ­¥éª¤ï¼Œä½¿ç”¨é¢„è®¾æŸ¥è¯¢
            if any(default in step_title.lower() for default in ["å¸‚åœºæ¦‚è§ˆ", "ç«äº‰åˆ†æ", "å…³é”®é©±åŠ¨å› ç´ ", "è¶‹åŠ¿å±•æœ›"]):
                return self._get_default_queries_for_step(step_title, topic)
            
            # ä½¿ç”¨AIç”ŸæˆæŸ¥è¯¢
            logger.info(f"ä¸ºæ­¥éª¤ '{step_title}' ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œä¸»é¢˜: {topic}")
            from app.services.siliconflow_service import siliconflow_service
            
            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„æç¤ºï¼Œå¼ºè°ƒå¿…é¡»åŒ…å«å…·ä½“ç ”ç©¶ä¸»é¢˜
            prompt = f"""è¯·ä¸ºä¸»é¢˜"{topic}"çš„ç ”ç©¶æ­¥éª¤"{step_title}"ç”Ÿæˆ 3 ä¸ªæœç´¢æŸ¥è¯¢ã€‚

éœ€æ±‚:
1. æ¯ä¸ªæŸ¥è¯¢å¿…é¡»åŒ…å«"{topic}"è¿™ä¸ªå…·ä½“ä¸»é¢˜è¯
2. ä¸è¦ä½¿ç”¨æŠ½è±¡æœ¯è¯­ä¾‹å¦‚â€œç ”ç©¶é—®é¢˜â€â€œå¸‚åœºåˆ†æâ€ç­‰å®½æ³›è¯æ±‡
3. æ¯ä¸ªæŸ¥è¯¢åº”è¯¥åŒ…å«å…·ä½“ã€ç›´æ¥ç›¸å…³çš„å…³é”®è¯
4. æŸ¥è¯¢é•¿åº¦ä¸è¶…è¿‡5ä¸ªå…³é”®è¯
5. åªè¾“å‡ºæŸ¥è¯¢è¯ï¼Œä¸è¦åŠ å…¥æ•°å­—ç¼–å·æˆ–å…¶ä»–æ ¼å¼åŒ–æ–‡æœ¬

æ­¥éª¤æè¿°: {step_description}

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
{topic} å¸‚åœºè§„æ¨¡ 2025
{topic} æœ€æ–°æ•°æ® åˆ†æ
{topic} è¡Œä¸šé¢†å…ˆä¼ä¸š
"""

            # å°è¯•è°ƒç”¨AIç”ŸæˆæŸ¥è¯¢
            queries = []
            try:
                search_queries_text = siliconflow_service.generate_search_queries(prompt, step_title)
                
                # åˆ†è¡Œå¹¶è¿‡æ»¤ç©ºè¡Œ
                raw_queries = [q.strip() for q in search_queries_text.split('\n') if q.strip()]
                
                # è¿‡æ»¤æ›¿æ¢æç¤ºä¸­çš„ç¤ºä¾‹æŸ¥è¯¢
                example_pattern = f"{topic} \u5e02\u573a\u89c4\u6a21 2025"
                queries = [q for q in raw_queries if q != example_pattern and len(q) < 100 and not q.startswith('#')]
                
                # å¤„ç†æ¯ä¸ªæŸ¥è¯¢ï¼Œç¡®ä¿åŒ…å«ç ”ç©¶ä¸»é¢˜
                for i in range(len(queries)):
                    if topic.lower() not in queries[i].lower():
                        queries[i] = f"{topic} {queries[i]}"
                
                logger.info(f"AIç”Ÿæˆäº† {len(queries)} ä¸ªæœç´¢æŸ¥è¯¢")
                
            except Exception as query_error:
                logger.warning(f"è°ƒç”¨æŸ¥è¯¢ç”ŸæˆæœåŠ¡å‡ºé”™: {str(query_error)}")
            
            # å¦‚æœAIç”Ÿæˆçš„æŸ¥è¯¢ä¸è¶³ä¸‰ä¸ªï¼Œè¡¥å……é»˜è®¤æŸ¥è¯¢
            if len(queries) < 3:
                # è®¡ç®—éœ€è¦æ·»åŠ çš„é»˜è®¤æŸ¥è¯¢æ•°é‡
                needed = 3 - len(queries)
                queries.extend(default_queries[:needed])
                logger.info(f"æ·»åŠ äº† {needed} ä¸ªé»˜è®¤æŸ¥è¯¢")
            
            # å»é™¤é‡å¤æŸ¥è¯¢
            unique_queries = []
            for q in queries:
                normalized_q = q.lower().strip()
                if normalized_q not in [uq.lower().strip() for uq in unique_queries]:
                    unique_queries.append(q)
            
            # ç¡®ä¿æ‰€æœ‰æŸ¥è¯¢éƒ½åŒ…å«ç ”ç©¶ä¸»é¢˜
            final_queries = []
            for q in unique_queries:
                if topic.lower() in q.lower():
                    final_queries.append(q)
                else:
                    final_queries.append(f"{topic} {q}")
            
            logger.info(f"æœ€ç»ˆç”Ÿæˆäº† {len(final_queries)} ä¸ªæœç´¢æŸ¥è¯¢: {final_queries[:3]}")
            
            # åªè¿”å›å‰ä¸‰ä¸ªæŸ¥è¯¢
            return final_queries[:3]
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœç´¢æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
            
            # å¦‚æœå‡ºé”™ï¼Œè¿”å›å¿…å®šåŒ…å«ä¸»é¢˜çš„åŸºæœ¬æŸ¥è¯¢
            return [
                f"{topic} {step_title}", 
                f"{topic} æœ€æ–°æ•°æ® åˆ†æ", 
                f"{topic} ç ”ç©¶æŠ¥å‘Š 2025"
            ]

            
    def _generate_search_queries(self, topic, requirements):

        """åŸºäºä¸»é¢˜å’Œè¦æ±‚ç”Ÿæˆæœç´¢æŸ¥è¯¢"""

        # åˆå§‹åŒ–æŸ¥è¯¢åˆ—è¡¨
        queries = []
        
        # åŸºç¡€æŸ¥è¯¢ - ä¸»é¢˜æœ¬èº«
        queries.append(topic)
        
        # ç‰¹å®šé¢†åŸŸæŸ¥è¯¢
        if "æ±½è½¦" in topic or "ç”µåŠ¨æ±½è½¦" in topic or "æ–°èƒ½æº" in topic:
            queries.extend([
                f"{topic} å¸‚åœºè§„æ¨¡ æ•°æ®",
                f"{topic} è¡Œä¸šè¶‹åŠ¿åˆ†æ",
                f"{topic} é”€é‡æ’å"
            ])
            
            # å¦‚æœæ˜¯ç”µåŠ¨æ±½è½¦ï¼Œæ·»åŠ ä¸€äº›ç‰¹å®šæŸ¥è¯¢
            if "ç”µåŠ¨æ±½è½¦" in topic:
                queries.extend([
                    f"ä¸­å›½ç”µåŠ¨æ±½è½¦ è¡¥è´´æ”¿ç­–",
                    f"ä¸­å›½ç”µåŠ¨æ±½è½¦ å……ç”µåŸºç¡€è®¾æ–½"
                ])
        elif "å¸‚åœº" in topic or "åˆ†æ" in topic:
            queries.extend([
                f"{topic} è¡Œä¸šæŠ¥å‘Š",
                f"{topic} æ•°æ®ç»Ÿè®¡",
                f"{topic} å‘å±•å‰æ™¯"
            ])
        elif "æŠ€æœ¯" in topic or "åˆ›æ–°" in topic:
            queries.extend([
                f"{topic} æœ€æ–°è¿›å±•",
                f"{topic} ç ”ç©¶è¿›å±•",
                f"{topic} æ¡ˆä¾‹åˆ†æ"
            ])
        else:
            # é€šç”¨çš„æŸ¥è¯¢
            queries.extend([
                f"{topic} æ•°æ®åˆ†æ",
                f"{topic} è¡Œä¸šè¶‹åŠ¿",
                f"{topic} å‘å±•ç°çŠ¶"
            ])
        
        # å¦‚æœæœ‰ç‰¹å®šè¦æ±‚ï¼Œæ·»åŠ ç›¸å…³æŸ¥è¯¢
        if requirements:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å®šæ—¶é—´èŒƒå›´
            time_patterns = ["è¿‘äº”å¹´", "è¿‘ä¸‰å¹´", "è¿‘åå¹´", "2023", "2022", "2021", "2020"]
            for pattern in time_patterns:
                if pattern in requirements:
                    queries.append(f"{topic} {pattern} æ•°æ®")
                    break
            
            # å¦‚æœæœ‰å…·ä½“çš„åˆ†æéœ€æ±‚
            if "é¢„æµ‹" in requirements or "å±•æœ›" in requirements:
                queries.append(f"{topic} æœªæ¥é¢„æµ‹")
            
            if "ä¼ä¸š" in requirements or "å…¬å¸" in requirements or "å“ç‰Œ" in requirements:
                queries.append(f"{topic} ä¸»è¦ä¼ä¸š å¸‚åœºä»½é¢")
        
        # å»é™¤é‡å¤çš„æŸ¥è¯¢
        unique_queries = list(set(queries))
        
        # é™åˆ¶æŸ¥è¯¢æ•°é‡ï¼Œé¿å…è¿‡å¤š
        return unique_queries[:5]


# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
research_service = ResearchService()
