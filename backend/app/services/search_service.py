import os
import requests
import json
import time
import re
from bs4 import BeautifulSoup
from serpapi.google_search import GoogleSearch
from datetime import datetime
import nltk
from nltk.tokenize import sent_tokenize
from urllib.parse import urlparse
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å°è¯•ä¸‹è½½NLTKæ•°æ®
nltk_data_downloaded = False

def ensure_nltk_data():
    """
    ç¡®ä¿NLTKæ•°æ®å·²ä¸‹è½½ï¼Œè®¾ç½®ä¸‹è½½è·¯å¾„ä¸ºå½“å‰ç›®å½•
    """
    global nltk_data_downloaded
    
    if nltk_data_downloaded:
        return
        
    try:
        # è®¾ç½®ä¸‹è½½ç›®å½•
        import os
        nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')
        os.makedirs(nltk_data_path, exist_ok=True)
        nltk.data.path.append(nltk_data_path)
        
        # ä¸‹è½½æ•°æ®
        logger.info(f"æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®åˆ° {nltk_data_path}")
        nltk.download('punkt', download_dir=nltk_data_path, quiet=False)
        nltk_data_downloaded = True
        logger.info("NLTKæ•°æ®ä¸‹è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"æ— æ³•ä¸‹è½½NLTKæ•°æ®: {str(e)}")

# åˆå§‹åŒ–æ—¶å°è¯•ä¸‹è½½
try:
    ensure_nltk_data()
except Exception as e:
    logger.warning(f"åˆå§‹åŒ–NLTKæ•°æ®æ—¶å‡ºé”™: {str(e)}")

class SearchService:
    """çœŸå®æœç´¢æœåŠ¡ï¼Œä½¿ç”¨SerpAPIå’Œç½‘é¡µçˆ¬å–æ¥è·å–çœŸå®æ•°æ®"""
    
    def __init__(self, api_key=None):
        self.serpapi_key = api_key or os.getenv("SERPAPI_API_KEY")
        if not self.serpapi_key:
            logger.warning("æœªè®¾ç½®SERPAPI_API_KEYï¼Œæœç´¢åŠŸèƒ½å°†å—é™")
            self.serpapi_key = None
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def search(self, query, num_results=30):
        """æ‰§è¡Œæœç´¢å¹¶è¿”å›ç»“æœ"""
        if self.serpapi_key:
            return self._search_with_serpapi(query, num_results)
        else:
            # å¦‚æœæ²¡æœ‰SerpAPIå¯†é’¥ï¼Œä½¿ç”¨å¤‡ç”¨çš„æœç´¢æ–¹æ³•
            return self._search_fallback(query, num_results)
    
    def _search_with_serpapi(self, query, num_results=30):
        """ä½¿ç”¨SerpAPIæ‰§è¡ŒGoogleæœç´¢"""
        try:
            params = {
                "engine": "google",
                "q": query,
                "api_key": self.serpapi_key,
                "num": num_results,
                "hl": "zh-cn"  # è®¾ç½®è¯­è¨€ä¸ºä¸­æ–‡
            }
            
            search = GoogleSearch(params)
            results = search.get_dict()
            
            if "error" in results:
                logger.error(f"SerpAPIé”™è¯¯: {results['error']}")
                return []
            
            organic_results = results.get("organic_results", [])
            
            search_results = []
            for result in organic_results[:num_results]:
                search_results.append({
                    "title": result.get("title", ""),
                    "link": result.get("link", ""),
                    "snippet": result.get("snippet", ""),
                    "source": self._get_domain_name(result.get("link", "")),
                    "source_icon": self._get_favicon(result.get("link", ""))
                })
            
            return search_results
        except Exception as e:
            logger.error(f"SerpAPIæœç´¢é”™è¯¯: {str(e)}")
            return []
    
    def _search_fallback(self, query, num_results=30):
        """å¤‡ç”¨æœç´¢æ–¹æ³•ï¼ˆå¦‚æœæ²¡æœ‰SerpAPIå¯†é’¥ï¼‰"""
        try:
            # ä½¿ç”¨DuckDuckGo APIï¼ˆä¸éœ€è¦APIå¯†é’¥ï¼‰
            url = f"https://api.duckduckgo.com/?q={query}&format=json"
            response = requests.get(url)
            data = response.json()
            
            search_results = []
            for result in data.get("RelatedTopics", [])[:num_results]:
                if "Text" in result and "FirstURL" in result:
                    search_results.append({
                        "title": result["Text"].split(" - ")[0] if " - " in result["Text"] else result["Text"],
                        "link": result["FirstURL"],
                        "snippet": result["Text"],
                        "source": self._get_domain_name(result["FirstURL"]),
                        "source_icon": self._get_favicon(result["FirstURL"])
                    })
            
            return search_results
        except Exception as e:
            logger.error(f"å¤‡ç”¨æœç´¢é”™è¯¯: {str(e)}")
            return []
    
    def fetch_content(self, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•æå–ä¸»è¦å†…å®¹
            main_content = ""
            
            # å°è¯•æŸ¥æ‰¾æ–‡ç« ä¸»ä½“
            for tag in ["article", "main", ".content", "#content", ".post", ".article"]:
                content = soup.select(tag)
                if content:
                    main_content = content[0].get_text(separator=' ', strip=True)
                    break
            
            # å¦‚æœæœªæ‰¾åˆ°ä¸»è¦å†…å®¹ï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰æ®µè½
            if not main_content:
                paragraphs = soup.find_all('p')
                main_content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            
            # å¦‚æœä¾ç„¶æ²¡æœ‰å†…å®¹ï¼Œåˆ™ä½¿ç”¨æ•´ä¸ªé¡µé¢æ–‡æœ¬
            if not main_content:
                main_content = soup.get_text(separator=' ', strip=True)
            
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if len(main_content) > 10000:
                main_content = main_content[:10000] + "..."
            
            return main_content
        except Exception as e:
            logger.error(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥ {url}: {str(e)}")
            return ""
    
    def extract_key_information(self, text, query):
        """ä»æ–‡æœ¬ä¸­æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯"""
        try:
            # å¦‚æœæ–‡æœ¬ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œç›´æ¥è¿”å›noneï¼Œé¿å…å¤„ç†ç©ºå†…å®¹
            if not text or len(text) < 50:
                logger.warning(f"æå–å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º: {len(text) if text else 0} å­—ç¬¦")
                return None
                
            # å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­
            try:
                sentences = sent_tokenize(text)
            except Exception as e:
                logger.warning(f"NLTKåˆ†å¥å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†å‰²: {str(e)}")
                # å¦‚æœNLTKåˆ†å¥å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬çš„åˆ†éš”ç¬¦
                sentences = [s.strip() for s in re.split(r'[.!?\n]', text) if s.strip()]
            
            # å¦‚æœä»ç„¶æ²¡æœ‰å¥å­ï¼Œè¿”å›none
            if not sentences:
                logger.warning("æ— æ³•æå–æœ‰æ•ˆçš„å¥å­")
                return None
            
            # ä¸ºæŸ¥è¯¢åˆ›å»ºå…³é”®è¯åˆ—è¡¨ï¼Œæ·»åŠ åˆ†è¯å¤„ç†
            query_parts = query.lower().replace('-', ' ').replace('_', ' ')
            query_keywords = [word for word in query_parts.split() if len(word) > 1]
            
            # å¯»æ‰¾åŒ…å«æŸ¥è¯¢å…³é”®è¯çš„å¥å­ï¼Œæ·»åŠ ç›¸å…³æ€§è¯„åˆ†
            relevant_sentences = []
            for sentence in sentences:
                sentence_lower = sentence.lower()
                # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                relevance = sum(1 for keyword in query_keywords if keyword in sentence_lower)
                if relevance > 0:
                    relevant_sentences.append((sentence, relevance))
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            relevant_sentences.sort(key=lambda x: x[1], reverse=True)
            
            # å¦‚æœæ‰¾åˆ°äº†ç›¸å…³å¥å­ï¼Œåªä¿ç•™å¥å­å†…å®¹
            relevant_sentences = [s[0] for s in relevant_sentences]
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å¥å­ï¼Œä½¿ç”¨å‰å‡ ä¸ªå¥å­
            if not relevant_sentences and sentences:
                relevant_sentences = sentences[:5]
                logger.info("æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é¦–å¥æ‘˜è¦")
            
            # å°†ç›¸å…³å¥å­ç»„åˆæˆæ®µè½
            if relevant_sentences:
                # é™åˆ¶è¿”å›çš„å¥å­æ•°é‡ï¼Œé¿å…è¿‡å¤š
                if len(relevant_sentences) > 8:
                    relevant_sentences = relevant_sentences[:8]
                    
                key_info = " ".join(relevant_sentences)
                # é™åˆ¶é•¿åº¦
                if len(key_info) > 1000:
                    key_info = key_info[:1000] + "..."
                return key_info
            else:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯")
                return None
        except Exception as e:
            logger.error(f"æå–å…³é”®ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None  # è¿”å›Noneè€Œä¸æ˜¯é”™è¯¯æ¶ˆæ¯
    
    def analyze_data(self, findings):
        """åˆ†ææ”¶é›†åˆ°çš„ç ”ç©¶å‘ç°ï¼ˆçº¯Pythonå®ç°ï¼‰"""
        try:
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å‘ç°ï¼Œåˆ™æ— æ³•åˆ†æ
            if len(findings) < 2:
                return "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†åˆ†æ"
            
            # æå–å¯èƒ½çš„æ•°å­—æ•°æ®
            numeric_data = []
            for finding in findings:
                # å°è¯•æå–ç™¾åˆ†æ¯”
                if "%" in finding:
                    parts = finding.split("%")
                    for i, part in enumerate(parts[:-1]):  # ä¸åŒ…æ‹¬æœ€åä¸€éƒ¨åˆ†ï¼ˆ%ç¬¦å·åé¢ï¼‰
                        words = part.split()
                        if words:
                            try:
                                value = float(words[-1])
                                numeric_data.append({
                                    "value": value,
                                    "type": "percentage",
                                    "context": finding
                                })
                            except:
                                pass
            
            # å¦‚æœæœ‰æ•°å€¼æ•°æ®ï¼Œç”Ÿæˆç®€å•çš„ç»Ÿè®¡åˆ†æ
            if numeric_data:
                values = [item["value"] for item in numeric_data]
                analysis = f"æ•°æ®åˆ†æï¼šä»æ”¶é›†çš„{len(findings)}æ¡ä¿¡æ¯ä¸­ï¼Œæˆ‘ä»¬æå–äº†{len(numeric_data)}ä¸ªæ•°å€¼æ•°æ®ç‚¹ã€‚"
                
                if len(values) >= 2:
                    avg_value = sum(values) / len(values)
                    max_value = max(values)
                    min_value = min(values)
                    analysis += f" è¿™äº›æ•°å€¼çš„å¹³å‡å€¼ä¸º{avg_value:.2f}%ï¼Œæœ€å¤§å€¼ä¸º{max_value:.2f}%ï¼Œæœ€å°å€¼ä¸º{min_value:.2f}%ã€‚"
                
                # æ·»åŠ ä¸€äº›å‘ç°çš„ä¸Šä¸‹æ–‡
                if numeric_data:
                    analysis += f" é‡è¦æ•°æ®ç‚¹åŒ…æ‹¬ï¼š{numeric_data[0]['context']}"
                
                return analysis
            else:
                # å¦‚æœæ²¡æœ‰æå–åˆ°æ•°å­—ï¼Œè¿”å›æ–‡æœ¬åˆ†æ
                total_text = " ".join(findings)
                common_phrases = self._extract_common_phrases(total_text)
                
                if common_phrases:
                    return f"æ–‡æœ¬åˆ†æï¼šåŸºäºæ”¶é›†çš„{len(findings)}æ¡ä¿¡æ¯ï¼Œæˆ‘ä»¬å‘ç°æœ€å¸¸å‡ºç°çš„ä¸»é¢˜æ˜¯{', '.join(common_phrases[:3])}ã€‚"
                else:
                    return f"æ–‡æœ¬åˆ†æï¼šåŸºäºæ”¶é›†çš„{len(findings)}æ¡ä¿¡æ¯ï¼Œæœªèƒ½æå–å‡ºæ˜ç¡®çš„ä¸»é¢˜æ¨¡å¼ã€‚"
        except Exception as e:
            logger.error(f"åˆ†ææ•°æ®å¤±è´¥: {str(e)}")
            return "åˆ†ææ•°æ®æ—¶å‡ºé”™"
    
    def _extract_common_phrases(self, text, num_phrases=3):
        """ä»æ–‡æœ¬ä¸­æå–å¸¸è§çŸ­è¯­"""
        try:
            # ç®€å•å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„NLPå¤„ç†
            words = text.lower().split()
            word_counts = {}
            
            for word in words:
                if len(word) > 2:  # å¿½ç•¥çŸ­è¯
                    word_counts[word] = word_counts.get(word, 0) + 1
            
            # è·å–æœ€å¸¸è§çš„è¯
            sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
            return [word for word, count in sorted_words[:num_phrases]]
        except Exception as e:
            logger.error(f"æå–å¸¸è§çŸ­è¯­å¤±è´¥: {str(e)}")
            return ["æœªèƒ½æå–å¸¸è§çŸ­è¯­"]
    
    def _get_domain_name(self, url):
        """ä»URLä¸­æå–åŸŸå"""
        try:
            domain = urlparse(url).netloc
            # ç§»é™¤"www."å‰ç¼€
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return url
    
    def _get_favicon(self, url):
        """ç”Ÿæˆç½‘ç«™å›¾æ ‡ä¿¡æ¯"""
        domain = self._get_domain_name(url).lower()
        
        # æ ¹æ®åŸŸåè¿”å›åˆé€‚çš„å›¾æ ‡Emoji
        if 'gov.cn' in domain:
            return 'ğŸ›ï¸'
        elif 'edu.cn' in domain or 'edu' in domain:
            return 'ğŸ“'
        elif 'org' in domain:
            return 'ğŸŒ'
        elif 'news' in domain or 'sina' in domain or 'sohu' in domain:
            return 'ğŸ“°'
        elif 'stats' in domain:
            return 'ğŸ“Š'
        elif 'byd' in domain or 'tesla' in domain or 'nio' in domain:
            return 'ğŸš—'
        elif 'ev' in domain or 'electric' in domain:
            return 'âš¡'
        elif 'caam' in domain:
            return 'ğŸ¢'
        else:
            return 'ğŸ”'

# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
search_service = SearchService()
