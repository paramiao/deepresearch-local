import os
import requests
import json
import time
import re
import html
import os
from bs4 import BeautifulSoup
from serpapi.google_search import GoogleSearch
from datetime import datetime
import nltk
from nltk.tokenize import sent_tokenize
from urllib.parse import urlparse
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å°è¯•ä¸‹è½½NLTKæ•°æ®
nltk_data_downloaded = False

def ensure_nltk_data():
    """
    ç¡®ä¿NLTKæ•°æ®å·²ä¸‹è½½ï¼Œè®¾ç½®ä¸‹è½½è·¯å¾„ä¸ºå½“å‰ç›®å½•
    """
    global nltk_data_downloaded
    
    if nltk_data_downloaded:
        return
        
    try:
        # è®¾ç½®ä¸‹è½½ç›®å½•
        import os
        nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')
        os.makedirs(nltk_data_path, exist_ok=True)
        nltk.data.path.append(nltk_data_path)
        
        # ä¸‹è½½æ•°æ®
        logger.info(f"æ­£åœ¨ä¸‹è½½NLTK punktæ•°æ®åˆ° {nltk_data_path}")
        nltk.download('punkt', download_dir=nltk_data_path, quiet=False)
        nltk_data_downloaded = True
        logger.info("NLTKæ•°æ®ä¸‹è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"æ— æ³•ä¸‹è½½NLTKæ•°æ®: {str(e)}")

# åˆå§‹åŒ–æ—¶å°è¯•ä¸‹è½½
try:
    ensure_nltk_data()
except Exception as e:
    logger.warning(f"åˆå§‹åŒ–NLTKæ•°æ®æ—¶å‡ºé”™: {str(e)}")

class SearchService:
    """çœŸå®æœç´¢æœåŠ¡ï¼Œä½¿ç”¨SerpAPIå’Œç½‘é¡µçˆ¬å–æ¥è·å–çœŸå®æ•°æ®"""
    
    def __init__(self, api_key=None):
        self.serpapi_key = api_key or os.getenv("SERPAPI_API_KEY")
        if not self.serpapi_key:
            logger.warning("æœªè®¾ç½®SERPAPI_API_KEYï¼Œæœç´¢åŠŸèƒ½å°†å—é™")
            self.serpapi_key = None
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def search(self, query, num_results=30):
        """æ‰§è¡Œæœç´¢å¹¶è¿”å›ç»“æœï¼Œå¤„ç†å¯èƒ½çš„ç¼–ç é—®é¢˜"""
        logger.info(f"æ‰§è¡Œæœç´¢æŸ¥è¯¢: {query}")
        try:
            # ç¡®ä¿æŸ¥è¯¢æ–‡æœ¬ç¼–ç æ­£ç¡®
            normalized_query = self._normalize_text(query) if hasattr(self, '_normalize_text') else query
            logger.info(f"è§„èŒƒåŒ–åçš„æŸ¥è¯¢: {normalized_query}")
            
            # æ‰§è¡Œæœç´¢
            if self.serpapi_key:
                results = self._search_with_serpapi(normalized_query, num_results)
            else:
                # å¦‚æœæ²¡æœ‰SerpAPIå¯†é’¥ï¼Œä½¿ç”¨å¤‡ç”¨çš„æœç´¢æ–¹æ³•
                results = self._search_fallback(normalized_query, num_results)
            
            # å¯¹ç»“æœè¿›è¡Œç¼–ç å¤„ç†
            for result in results:
                if 'title' in result and hasattr(self, '_normalize_text'):
                    result['title'] = self._normalize_text(result['title'])
                if 'snippet' in result and hasattr(self, '_normalize_text'):
                    result['snippet'] = self._normalize_text(result['snippet'])
            
            return results
        except Exception as e:
            logger.error(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            return []
    
    def _search_with_serpapi(self, query, num_results=30):
        """ä½¿ç”¨SerpAPIæ‰§è¡ŒGoogleæœç´¢"""
        try:
            params = {
                "engine": "google",
                "q": query,
                "api_key": self.serpapi_key,
                "num": num_results,
                "hl": "zh-cn"  # è®¾ç½®è¯­è¨€ä¸ºä¸­æ–‡
            }
            
            search = GoogleSearch(params)
            results = search.get_dict()
            
            if "error" in results:
                logger.error(f"SerpAPIé”™è¯¯: {results['error']}")
                return []
            
            organic_results = results.get("organic_results", [])
            
            search_results = []
            for result in organic_results[:num_results]:
                search_results.append({
                    "title": result.get("title", ""),
                    "link": result.get("link", ""),
                    "snippet": result.get("snippet", ""),
                    "source": self._get_domain_name(result.get("link", "")),
                    "source_icon": self._get_favicon(result.get("link", ""))
                })
            
            return search_results
        except Exception as e:
            logger.error(f"SerpAPIæœç´¢é”™è¯¯: {str(e)}")
            return []
    
    def _search_fallback(self, query, num_results=30):
        """å¤‡ç”¨æœç´¢æ–¹æ³•ï¼ˆå¦‚æœæ²¡æœ‰SerpAPIå¯†é’¥ï¼‰"""
        try:
            # ä½¿ç”¨DuckDuckGo APIï¼ˆä¸éœ€è¦APIå¯†é’¥ï¼‰
            url = f"https://api.duckduckgo.com/?q={query}&format=json"
            response = requests.get(url)
            data = response.json()
            
            search_results = []
            for result in data.get("RelatedTopics", [])[:num_results]:
                if "Text" in result and "FirstURL" in result:
                    search_results.append({
                        "title": result["Text"].split(" - ")[0] if " - " in result["Text"] else result["Text"],
                        "link": result["FirstURL"],
                        "snippet": result["Text"],
                        "source": self._get_domain_name(result["FirstURL"]),
                        "source_icon": self._get_favicon(result["FirstURL"])
                    })
            
            return search_results
        except Exception as e:
            logger.error(f"å¤‡ç”¨æœç´¢é”™è¯¯: {str(e)}")
            return []
    
    def fetch_content(self, url):
        """è·å–ç½‘é¡µå†…å®¹ï¼Œå¹¶å¤„ç†ç¼–ç é—®é¢˜"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br'
            }
            response = self.session.get(url, timeout=10, headers=headers)
            response.raise_for_status()
            
            # å°è¯•æ£€æµ‹å†…å®¹ç¼–ç 
            if response.encoding.lower() == 'iso-8859-1':
                # å¯èƒ½æ˜¯è¯¯åˆ¤ï¼Œå°è¯•ç”¨æ›´å¯èƒ½çš„ç¼–ç 
                encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'big5']
                for enc in encodings:
                    try:
                        text = response.content.decode(enc)
                        response.encoding = enc
                        logger.info(f"æˆåŠŸä½¿ç”¨ {enc} ç¼–ç è§£æç½‘é¡µå†…å®¹")
                        break
                    except UnicodeDecodeError:
                        continue
            
            # ä½¿ç”¨æ­£ç¡®çš„ç¼–ç å’ŒHTMLè§£æå™¨
            soup = BeautifulSoup(response.content, 'html.parser', from_encoding=response.encoding)
            
            # å°è¯•æå–ä¸»è¦å†…å®¹
            main_content = ""
            
            # å°è¯•æŸ¥æ‰¾æ–‡ç« ä¸»ä½“
            for tag in ["article", "main", ".content", "#content", ".post", ".article"]:
                content = soup.select(tag)
                if content:
                    main_content = content[0].get_text(separator=' ', strip=True)
                    break
            
            # å¦‚æœæœªæ‰¾åˆ°ä¸»è¦å†…å®¹ï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰æ®µè½
            if not main_content:
                paragraphs = soup.find_all('p')
                main_content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            
            # å¦‚æœä¾ç„¶æ²¡æœ‰å†…å®¹ï¼Œåˆ™ä½¿ç”¨æ•´ä¸ªé¡µé¢æ–‡æœ¬
            if not main_content:
                main_content = soup.get_text(separator=' ', strip=True)
            
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if len(main_content) > 10000:
                main_content = main_content[:10000] + "..."
            
            return main_content
        except Exception as e:
            logger.error(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥ {url}: {str(e)}")
            return ""
    
    def extract_key_information(self, text, query):
        """ä»æ–‡æœ¬ä¸­æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯ - æ”¹è¿›ç‰ˆæœ¬"""
        try:
            # å¦‚æœæ–‡æœ¬ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œç›´æ¥è¿”å›noneï¼Œé¿å…å¤„ç†ç©ºå†…å®¹
            if not text or len(text) < 50:
                logger.warning(f"æå–å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º: {len(text) if text else 0} å­—ç¬¦")
                return None
            
            # é¢„æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºæ ¼å’Œå‡ ä¸ªå¸¸è§çš„æ— ç”¨æ–‡æœ¬æ¨¡å¼
            text = re.sub(r'\s+', ' ', text)  # å°†å¤šä¸ªç©ºç™½å­—ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
            text = re.sub(r'(\d+)\s*?[.:]\s*', r'\1. ', text)  # ä¿®å¤æ•°å­—åˆ—è¡¨æ ¼å¼
            text = re.sub(r'\.(\w)', r'. \1', text)  # ä¿®å¤å¥ç‚¹åç¼ºå°‘ç©ºæ ¼çš„é—®é¢˜
                
            # å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­ - ä½¿ç”¨å¤šç§ç­–ç•¥
            sentences = []
            
            # å…ˆå°è¯•ä½¿ç”¨NLTK
            try:
                ensure_nltk_data()  # ç¡®ä¿æ•°æ®å·²ä¸‹è½½
                sentences = sent_tokenize(text)
            except Exception as e:
                logger.warning(f"NLTKåˆ†å¥å¤±è´¥: {str(e)}")
            
            # å¦‚æœNLTKå¤±è´¥ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•
            if not sentences:
                logger.info("ä½¿ç”¨è‡ªå®šä¹‰åˆ†å¥è§„åˆ™")
                # ç¬¬ä¸€é˜¶æ®µï¼šå…ˆåˆ†å‰²æ®µè½
                paragraphs = re.split(r'\n\s*\n|\r\n\s*\r\n', text)
                
                # ç¬¬äºŒé˜¶æ®µï¼šå¯¹æ¯ä¸ªæ®µè½åˆ†å‰²å¥å­
                for paragraph in paragraphs:
                    # ä½¿ç”¨å¤šç§åˆ†å‰²ç¬¦å·åˆ†å‰²å¥å­
                    para_sentences = re.split(r'(?<=[.!?])\s+|(?<=[.!?])(?=[A-Z])|\.\s|\!\s|\?\s|\;\s|\n\s*\-|\n\s*\*|\n\s*\d+\.', paragraph)
                    # æ¸…ç†å¹¶æ·»åŠ æœ‰æ•ˆå¥å­
                    for s in para_sentences:
                        s = s.strip()
                        if s and len(s) > 15:  # åªåŠ å…¥è¶³å¤Ÿé•¿åº¦çš„æœ‰æ•ˆå¥å­
                            sentences.append(s)
            
            # å¦‚æœä»ç„¶æ²¡æœ‰æˆåŠŸæå–å¥å­ï¼Œä½¿ç”¨æœ€åŸå§‹çš„åˆ†éš”ç¬¦
            if not sentences and text:
                logger.warning("ä½¿ç”¨åŸºæœ¬åˆ†éš”ç¬¦åˆ†å¥")
                sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip() and len(s.strip()) > 15]
            
            # æœ€åæ‰‹æ®µï¼šå¦‚æœè¿˜æ˜¯æ— æ³•åˆ†å¥ï¼Œå°±æŒ‰å›ºå®šé•¿åº¦åˆ†å‰²
            if not sentences and len(text) > 100:
                logger.warning("æŒ‰å›ºå®šé•¿åº¦åˆ†å‰²æ–‡æœ¬")
                chunk_size = 150
                sentences = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
            
            # å¦‚æœä»ç„¶æ²¡æœ‰å¥å­ï¼Œè¿”å›åŸå§‹æ–‡æœ¬çš„ä¸€éƒ¨åˆ†
            if not sentences and text:
                return text[:500] + "..." if len(text) > 500 else text
            elif not sentences:
                logger.warning("æ— æ³•æå–æœ‰æ•ˆçš„å¥å­")
                return None
            
            # æ‹†åˆ†æŸ¥è¯¢å…³é”®è¯å’ŒçŸ­è¯­ - æ”¹è¿›ç‰ˆç®—æ³•
            query_parts = query.lower()
            # å¤„ç†å¤šç§åˆ†éš”ç¬¦
            query_parts = re.sub(r'[-_\s]', ' ', query_parts)
            
            # ä¼˜åŒ–å…³é”®è¯æå–ç®—æ³•
            # 1. æŒ‰ç©ºæ ¼åˆ’åˆ†
            space_split_keywords = [word for word in query_parts.split() if len(word) > 2]
            
            # 2. å°è¯•æå–çŸ­è¯­(è¿ç»­2-3ä¸ªè¯)
            phrases = []
            words = query_parts.split()
            if len(words) >= 2:
                for i in range(len(words)-1):
                    if len(words[i]) > 1 and len(words[i+1]) > 1:  # ç¡®ä¿ä¸æ˜¯å•ä¸ªå­—ç¬¦
                        phrases.append(f"{words[i]} {words[i+1]}")
                        
            # 3. ç‰¹æ®Šå¤„ç†ä¸­æ–‡æŸ¥è¯¢
            chinese_keywords = []
            if re.search(r'[\u4e00-\u9fa5]', query_parts):
                # ä¸­æ–‡æ–‡æœ¬å¤„ç†ï¼šæå–è¿ç»­2-4ä¸ªå­—ç¬¦
                for i in range(len(query_parts)):
                    if i+2 <= len(query_parts):
                        chunk2 = query_parts[i:i+2]
                        if re.search(r'[\u4e00-\u9fa5]', chunk2) and len(chunk2.strip()) > 1:
                            chinese_keywords.append(chunk2)
                    if i+3 <= len(query_parts):
                        chunk3 = query_parts[i:i+3]
                        if re.search(r'[\u4e00-\u9fa5]', chunk3) and len(chunk3.strip()) > 2:
                            chinese_keywords.append(chunk3)
                    if i+4 <= len(query_parts):
                        chunk4 = query_parts[i:i+4]
                        if re.search(r'[\u4e00-\u9fa5]', chunk4) and len(chunk4.strip()) > 3:
                            chinese_keywords.append(chunk4)
            
            # ç»„åˆæ‰€æœ‰æå–çš„å…³é”®è¯
            query_keywords = space_split_keywords + phrases + chinese_keywords
            
            # å»é™¤é‡å¤å¹¶è¿‡æ»¤å¸¸è§çš„æ— æ„ä¹‰è¯
            stopwords = ['ç ”ç©¶', 'é—®é¢˜', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'åˆ†æ']
            query_keywords = [k for k in query_keywords if k.strip() and k.strip().lower() not in stopwords]
            query_keywords = list(set(query_keywords))  # å»é™¤é‡å¤
            
            # å¦‚æœç»ˆç©¶æ²¡æœ‰æœ‰æ•ˆå…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢
            if not query_keywords:
                query_keywords = [query_parts]
            
            # å¯»æ‰¾ç›¸å…³å¥å­å¹¶æ‰“åˆ† - å…¨æ–°å¢å¼ºç‰ˆåŒ¹é…ç®—æ³•
            relevant_sentences = []
            
            # è®°å½•å·²å¤„ç†çš„å¥å­ä»¥é¿å…é‡å¤
            processed_sentences = set()
            
            for sentence in sentences:
                # è·³è¿‡è¿‡çŸ­æˆ–é‡å¤çš„å¥å­
                if len(sentence) < 20 or sentence in processed_sentences:
                    continue
                    
                processed_sentences.add(sentence)
                sentence_lower = sentence.lower()
                
                # å®šä¹‰ä¸åŒç±»å‹çš„åŒ¹é…å¾—åˆ†
                exact_match_score = 0     # ç²¾ç¡®åŒ¹é…å¾—åˆ†
                partial_match_score = 0    # éƒ¨åˆ†åŒ¹é…å¾—åˆ†
                semantic_match_score = 0    # è¯­ä¹‰ç›¸å…³å¾—åˆ†
                data_value_score = 0       # æ•°æ®ä»·å€¼å¾—åˆ†
                
                # 1. ç²¾ç¡®å…³é”®è¯åŒ¹é…
                for keyword in query_keywords:
                    # å®Œå…¨åŒ¹é…ï¼šå…³é”®è¯å‡ºç°åœ¨å¥å­ä¸­
                    if keyword in sentence_lower:
                        exact_match_score += 3
                        # ç»™ä¸­æ–‡çŸ­è¯­æ›´é«˜æƒé‡
                        if re.search(r'[\u4e00-\u9fa5]', keyword) and len(keyword) >= 3:
                            exact_match_score += 1
                    # éƒ¨åˆ†åŒ¹é…ï¼šå¥å­åŒ…å«å…³é”®è¯çš„ä¸€éƒ¨åˆ†
                    elif len(keyword) > 4 and any(part in sentence_lower for part in keyword.split() if len(part) > 3):
                        partial_match_score += 1
                    # è¯­ä¹‰ç›¸å…³åŒ¹é…ï¼šæ£€æµ‹å¥å­ä¸­æ˜¯å¦æœ‰ç›¸å…³è¯æ±‡
                    elif any(related in sentence_lower for related in self._get_related_terms(keyword)):
                        semantic_match_score += 0.5
                
                # 2. æ•°æ®è´¨é‡æƒé‡
                # æ£€æµ‹ç™¾åˆ†æ¯”æ•°æ®
                percentage_matches = re.findall(r'\d+(?:\.\d+)?\s*(?:%|ç™¾åˆ†ä¹‹|ç™¾åˆ†æ¯”)', sentence_lower)
                if percentage_matches:
                    data_value_score += len(percentage_matches) * 2
                
                # æ£€æµ‹å¹´ä»½å’Œæ—¥æœŸä¿¡æ¯
                date_matches = re.findall(r'\d{4}(?:\s*[å¹´-]\s*\d{1,2}\s*[æœˆæ—¥]?)', sentence_lower)
                if date_matches:
                    data_value_score += len(date_matches)
                
                # æ£€æµ‹æ•°å­—åˆ—è¡¨å’Œè¡¨æ ¼æ•°æ®
                if re.search(r'\d+\.\s*\w+|\([1-9]\)|[1-9]\)\s*\w+|\u7b2c[\u4e00-\u4e94å…­ä¸ƒå…«ä¹å]|\u8868\s*\d+', sentence_lower):
                    data_value_score += 1
                
                # é‡è¦ä¿¡æ¯æ ‡è®°
                info_markers = ['\u91cdè¦', '\u7279åˆ«', '\u503cå¾—\u6ce8æ„', '\u503cå¾—å…³æ³¨', '\u4e3bè¦', '\u5173é”®', 'important', 'critical', 'significant']
                if any(marker in sentence_lower for marker in info_markers):
                    data_value_score += 1
                
                # 3. è®¡ç®—åŠ æƒæ€»åˆ†
                total_score = exact_match_score * 1.5 + partial_match_score + semantic_match_score + data_value_score * 1.2
                
                # åªæ”¶é›†è¶…è¿‡é˜ˆå€¼çš„å¥å­
                if total_score > 0.5:  # é™ä½é˜ˆå€¼å¢åŠ å¯èƒ½çš„åŒ¹é…æ•°é‡
                    relevant_sentences.append((sentence, total_score))
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            relevant_sentences.sort(key=lambda x: x[1], reverse=True)
            
            # å¦‚æœæ‰¾åˆ°äº†ç›¸å…³å¥å­ï¼Œåªä¿ç•™å¥å­å†…å®¹
            relevant_sentences = [s[0] for s in relevant_sentences]
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å¥å­ï¼Œä½¿ç”¨å‰å‡ ä¸ªå¥å­
            if not relevant_sentences and sentences:
                relevant_sentences = sentences[:5]
                logger.info("æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é¦–å¥æ‘˜è¦")
            
            # å°†ç›¸å…³å¥å­ç»„åˆæˆæ®µè½
            if relevant_sentences:
                # é™åˆ¶è¿”å›çš„å¥å­æ•°é‡ï¼Œé¿å…è¿‡å¤š
                if len(relevant_sentences) > 8:
                    relevant_sentences = relevant_sentences[:8]
                    
                key_info = " ".join(relevant_sentences)
                # é™åˆ¶é•¿åº¦
                if len(key_info) > 1000:
                    key_info = key_info[:1000] + "..."
                return key_info
            else:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯")
                return None
        except Exception as e:
            logger.error(f"æå–å…³é”®ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None  # è¿”å›Noneè€Œä¸æ˜¯é”™è¯¯æ¶ˆæ¯
    
    def _get_related_terms(self, keyword):
        """ç”Ÿæˆä¸ç»™å®šå…³é”®è¯è¯­ä¹‰ç›¸å…³çš„è¯æ±‡
        
        Args:
            keyword: è¾“å…¥å…³é”®è¯
        
        Returns:
            åˆ—è¡¨: è¯­ä¹‰ç›¸å…³è¯æ±‡
        """
        # é€šç”¨å…³é”®è¯æ˜ å°„è¡¨
        common_term_map = {
            # ä¸­æ–‡å¸¸è§è¯­ä¹‰ç›¸å…³æ˜ å°„
            'ç ”ç©¶': ['åˆ†æ', 'è°ƒæŸ¥', 'è€ƒå¯Ÿ', 'å®éªŒ', 'æ¢ç´¢', 'æµ‹è¯•'],
            'å¸‚åœº': ['é”€å”®', 'è¡Œä¸š', 'å•†ä¸š', 'é”€é‡', 'å®¢æˆ·', 'æ¶ˆè´¹', 'å•†å“', 'äº§å“'],
            'åˆ†æ': ['è¯„ä¼°', 'è§£æ', 'æ¯”è¾ƒ', 'è¯„ä»·', 'è€ƒé‡', 'ç ”åˆ¤'],
            'æ•°æ®': ['ç»Ÿè®¡', 'ä¿¡æ¯', 'èµ„æ–™', 'è¡¨æ ¼', 'æŒ‡æ ‡', 'ç™¾åˆ†æ¯”', 'å æ¯”'],
            'è¶‹åŠ¿': ['å‘å±•', 'èµ°åŠ¿', 'å˜åŒ–', 'æ½®æµ', 'æœªæ¥', 'å¢é•¿', 'ä¸Šæ¶¨', 'ä¸‹é™'],
            'ç«äº‰': ['å¯¹æ‰‹', 'å¯¹æ¯”', 'æ¯”è¾ƒ', 'ç«äº‰è€…', 'ç«äº‰æ ¼å±€', 'ç«äº‰å¯¹æ‰‹'],
            'é—®é¢˜': ['å›°éš¾', 'éšœç¢', 'æŒ‘æˆ˜', 'ç“¶é¢ˆ', 'çŸ›ç›¾'],
            'å‘å±•': ['å¢é•¿', 'æ‰©å¤§', 'æå‡', 'æ”¹è¿›', 'è¿›æ­¥'],
            'æˆæœ¬': ['æ”¯å‡º', 'è´¹ç”¨', 'ä»·æ ¼', 'èŠ±è´¹', 'æŠ•å…¥'],
            'æ•ˆç›Š': ['å›æŠ¥', 'åˆ©æ¶¦', 'æ”¶ç›Š', 'æ•ˆç›Šç‡', 'æ•ˆç›Šç‡', 'æŠ•èµ„å›æŠ¥'],
            # è‹±æ–‡å¸¸è§è¯­ä¹‰ç›¸å…³æ˜ å°„
            'market': ['industry', 'business', 'commercial', 'customers', 'consumers', 'sales'],
            'analysis': ['evaluation', 'assessment', 'research', 'study', 'investigation', 'examination'],
            'data': ['statistics', 'figures', 'information', 'metrics', 'indicators', 'percentage'],
            'trend': ['development', 'movement', 'change', 'growth', 'future', 'evolution'],
            'cost': ['expense', 'expenditure', 'payment', 'investment', 'price'],
            'benefit': ['profit', 'return', 'gain', 'advantage', 'roi', 'return on investment']
        }

        # é¦–å…ˆæ£€æŸ¥å…³é”®è¯æ˜¯å¦ç›´æ¥åœ¨æ˜ å°„è¡¨ä¸­
        if keyword.lower() in common_term_map:
            return common_term_map[keyword.lower()]
            
        # æ£€æŸ¥å…³é”®è¯æ˜¯å¦æ˜¯å¬å›æ˜ å°„è¡¨ä¸­æŸä¸ªè¯çš„ä¸€éƒ¨åˆ†
        related_terms = []
        for base_word, terms in common_term_map.items():
            # æ£€æŸ¥å…³é”®è¯æ˜¯å¦åŒ…å«åœ¨ä»»ä½•åŸºç¡€è¯ä¸­
            if base_word in keyword or keyword in base_word:
                related_terms.extend(terms)
            # å¦‚æœå…³é”®è¯åœ¨ä»»ä½•ç›¸å…³è¯ä¸­
            for term in terms:
                if term in keyword or keyword in term:
                    # æ·»åŠ åŸºç¡€è¯åŠå…¶ä»–ç›¸å…³è¯
                    related_terms.append(base_word)
                    related_terms.extend([t for t in terms if t != term])
        
        # è¿”å›å»é‡åçš„ç»“æœ
        return list(set(related_terms))
    
    def _normalize_text(self, text):
        """æ¸…ç†å’Œè§„èŒƒåŒ–æ–‡æœ¬ï¼Œä¿®å¤ç¼–ç é—®é¢˜å’Œç‰¹æ®Šå­—ç¬¦"""
        if not text:
            return ""
            
        # æ›¿æ¢å¸¸è§çš„HTMLç¼–ç å­—ç¬¦
        text = html.unescape(text)
        
        # æ›¿æ¢å¸¸è§çš„æ— æ•ˆUTF-8åºåˆ—
        text = re.sub(r'\\u[0-9a-fA-F]{4}', ' ', text)
        text = re.sub(r'\\x[0-9a-fA-F]{2}', ' ', text)
        
        # åˆ é™¤æ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        
        # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def analyze_data(self, findings):
        """åˆ†ææ”¶é›†åˆ°çš„ç ”ç©¶å‘ç°ï¼ˆçº¯Pythonå®ç°ï¼‰"""
        try:
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å‘ç°ï¼Œåˆ™æ— æ³•åˆ†æ
            if len(findings) < 2:
                return "æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†åˆ†æ"
            
            # æå–å¯èƒ½çš„æ•°å­—æ•°æ®
            numeric_data = []
            for finding in findings:
                # å°è¯•æå–ç™¾åˆ†æ¯”
                if "%" in finding:
                    parts = finding.split("%")
                    for i, part in enumerate(parts[:-1]):  # ä¸åŒ…æ‹¬æœ€åä¸€éƒ¨åˆ†ï¼ˆ%ç¬¦å·åé¢ï¼‰
                        words = part.split()
                        if words:
                            try:
                                value = float(words[-1])
                                numeric_data.append({
                                    "value": value,
                                    "type": "percentage",
                                    "context": finding
                                })
                            except:
                                pass
            
            # å¦‚æœæœ‰æ•°å€¼æ•°æ®ï¼Œç”Ÿæˆç®€å•çš„ç»Ÿè®¡åˆ†æ
            if numeric_data:
                values = [item["value"] for item in numeric_data]
                analysis = f"æ•°æ®åˆ†æï¼šä»æ”¶é›†çš„{len(findings)}æ¡ä¿¡æ¯ä¸­ï¼Œæˆ‘ä»¬æå–äº†{len(numeric_data)}ä¸ªæ•°å€¼æ•°æ®ç‚¹ã€‚"
                
                if len(values) >= 2:
                    avg_value = sum(values) / len(values)
                    max_value = max(values)
                    min_value = min(values)
                    analysis += f" è¿™äº›æ•°å€¼çš„å¹³å‡å€¼ä¸º{avg_value:.2f}%ï¼Œæœ€å¤§å€¼ä¸º{max_value:.2f}%ï¼Œæœ€å°å€¼ä¸º{min_value:.2f}%ã€‚"
                
                # æ·»åŠ ä¸€äº›å‘ç°çš„ä¸Šä¸‹æ–‡
                if numeric_data:
                    analysis += f" é‡è¦æ•°æ®ç‚¹åŒ…æ‹¬ï¼š{numeric_data[0]['context']}"
                
                return analysis
            else:
                # å¦‚æœæ²¡æœ‰æå–åˆ°æ•°å­—ï¼Œè¿”å›æ–‡æœ¬åˆ†æ
                total_text = " ".join(findings)
                common_phrases = self._extract_common_phrases(total_text)
                
                if common_phrases:
                    return f"æ–‡æœ¬åˆ†æï¼šåŸºäºæ”¶é›†çš„{len(findings)}æ¡ä¿¡æ¯ï¼Œæˆ‘ä»¬å‘ç°æœ€å¸¸å‡ºç°çš„ä¸»é¢˜æ˜¯{', '.join(common_phrases[:3])}ã€‚"
                else:
                    return f"æ–‡æœ¬åˆ†æï¼šåŸºäºæ”¶é›†çš„{len(findings)}æ¡ä¿¡æ¯ï¼Œæœªèƒ½æå–å‡ºæ˜ç¡®çš„ä¸»é¢˜æ¨¡å¼ã€‚"
        except Exception as e:
            logger.error(f"åˆ†ææ•°æ®å¤±è´¥: {str(e)}")
            return "åˆ†ææ•°æ®æ—¶å‡ºé”™"
    
    def _extract_common_phrases(self, text, num_phrases=3):
        """ä»æ–‡æœ¬ä¸­æå–å¸¸è§çŸ­è¯­"""
        try:
            # ç®€å•å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„NLPå¤„ç†
            words = text.lower().split()
            word_counts = {}
            
            for word in words:
                if len(word) > 2:  # å¿½ç•¥çŸ­è¯
                    word_counts[word] = word_counts.get(word, 0) + 1
            
            # è·å–æœ€å¸¸è§çš„è¯
            sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
            return [word for word, count in sorted_words[:num_phrases]]
        except Exception as e:
            logger.error(f"æå–å¸¸è§çŸ­è¯­å¤±è´¥: {str(e)}")
            return ["æœªèƒ½æå–å¸¸è§çŸ­è¯­"]
    
    def _get_domain_name(self, url):
        """ä»URLä¸­æå–åŸŸå"""
        try:
            domain = urlparse(url).netloc
            # ç§»é™¤"www."å‰ç¼€
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return url
    
    def _get_favicon(self, url):
        """ç”Ÿæˆç½‘ç«™å›¾æ ‡ä¿¡æ¯"""
        domain = self._get_domain_name(url).lower()
        
        # æ ¹æ®åŸŸåè¿”å›åˆé€‚çš„å›¾æ ‡Emoji
        if 'gov.cn' in domain:
            return 'ğŸ›ï¸'
        elif 'edu.cn' in domain or 'edu' in domain:
            return 'ğŸ“'
        elif 'org' in domain:
            return 'ğŸŒ'
        elif 'news' in domain or 'sina' in domain or 'sohu' in domain:
            return 'ğŸ“°'
        elif 'stats' in domain:
            return 'ğŸ“Š'
        elif 'byd' in domain or 'tesla' in domain or 'nio' in domain:
            return 'ğŸš—'
        elif 'ev' in domain or 'electric' in domain:
            return 'âš¡'
        elif 'caam' in domain:
            return 'ğŸ¢'
        else:
            return 'ğŸ”'

# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
search_service = SearchService()
